# LMDeploy v0.10.0

```shell
python3 -m pip install lmdeploy==0.10.0
```

Serve a model through:

```shell
lmdeploy serve api_server $MODEL_REPO_ID --tp $TP --enable-metrics
```

<!-- toc -->

## Table of Contents

- [LMDeploy v0.10.0](#lmdeploy-v0100)
  - [Table of Contents](#table-of-contents)
  - [Turbomind Engine](#turbomind-engine)
    - [Qwen3-8B-FP8](#qwen3-8b-fp8)
    - [Qwen3-32B-FP8](#qwen3-32b-fp8)
    - [Qwen3-30B-A3B-FP8](#qwen3-30b-a3b-fp8)
    - [Qwen3-235B-A22B-FP8](#qwen3-235b-a22b-fp8)
    - [gpt-oss-20b](#gpt-oss-20b)
    - [gpt-oss-120b](#gpt-oss-120b)

<!-- tocstop -->

## Turbomind Engine

### Qwen3-8B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-8B-FP8 --tp 1 --enable-metrics
```

```{text}
$ bash benchmark_serving.sh
Using backend: lmdeploy
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (1.26.4)
Requirement already satisfied: transformers in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (4.55.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: async-timeout<6.0,>=4.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (6.6.3)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: typing-extensions>=4.1.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp) (4.14.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)
Requirement already satisfied: filelock in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (2025.7.33)
Requirement already satisfied: requests in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from requests->transformers) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.94s
██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:25<00:00, 68.80it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  145.35
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2001728
Request throughput (req/s):              68.80
Input token throughput (tok/s):          15539.44
Output token throughput (tok/s):         13706.52
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   72497.56
Median E2E Latency (ms):                 71765.64
---------------Time to First Token----------------
Mean TTFT (ms):                          61801.82
Median TTFT (ms):                        58942.94
P99 TTFT (ms):                           121672.04
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          58.90
Median TPOT (ms):                        62.93
P99 TPOT (ms):                           177.56
---------------Inter-token Latency----------------
Mean ITL (ms):                           113.18
Median ITL (ms):                         111.35
P99 ITL (ms):                            421.98
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 8.29s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:55<00:00,  6.77it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  295.52
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2048450
Request throughput (req/s):              6.77
Input token throughput (tok/s):          6930.18
Output token throughput (tok/s):         6930.18
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   168397.07
Median E2E Latency (ms):                 156488.16
---------------Time to First Token----------------
Mean TTFT (ms):                          125873.61
Median TTFT (ms):                        119273.27
P99 TTFT (ms):                           270188.57
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          41.57
Median TPOT (ms):                        36.31
P99 TPOT (ms):                           67.99
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.92
Median ITL (ms):                         30.33
P99 ITL (ms):                            169.29
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 50.58s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [17:51<00:00,  2.14s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1071.07
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998588
Request throughput (req/s):              0.47
Input token throughput (tok/s):          933.65
Output token throughput (tok/s):         2800.94
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   609003.89
Median E2E Latency (ms):                 610320.07
---------------Time to First Token----------------
Mean TTFT (ms):                          345326.61
Median TTFT (ms):                        315373.29
P99 TTFT (ms):                           786210.76
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          43.95
Median TPOT (ms):                        44.26
P99 TPOT (ms):                           75.99
---------------Inter-token Latency----------------
Mean ITL (ms):                           44.08
Median ITL (ms):                         26.52
P99 ITL (ms):                            130.98
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 17.18s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [09:02<00:00,  1.09s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  542.63
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000487
Request throughput (req/s):              0.92
Input token throughput (tok/s):          5528.61
Output token throughput (tok/s):         1842.87
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   303065.70
Median E2E Latency (ms):                 309633.73
---------------Time to First Token----------------
Mean TTFT (ms):                          238904.04
Median TTFT (ms):                        249310.36
P99 TTFT (ms):                           497098.99
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          32.10
Median TPOT (ms):                        30.09
P99 TPOT (ms):                           56.51
---------------Inter-token Latency----------------
Mean ITL (ms):                           32.13
Median ITL (ms):                         26.05
P99 ITL (ms):                            208.27
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-32B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-32B-FP8 --tp 1 --enable-metrics
```

```{text}
$ bash benchmark_serving.sh
Using backend: lmdeploy
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 5.34s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [07:49<00:00, 21.28it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  469.98
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2001094
Request throughput (req/s):              21.28
Input token throughput (tok/s):          4805.72
Output token throughput (tok/s):         4238.87
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   228757.22
Median E2E Latency (ms):                 228086.09
---------------Time to First Token----------------
Mean TTFT (ms):                          212529.28
Median TTFT (ms):                        209639.31
P99 TTFT (ms):                           436939.76
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          86.48
Median TPOT (ms):                        83.64
P99 TPOT (ms):                           147.98
---------------Inter-token Latency----------------
Mean ITL (ms):                           82.45
Median ITL (ms):                         72.32
P99 ITL (ms):                            204.80
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 22.65s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [17:01<00:00,  1.96it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1021.15
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2048882
Request throughput (req/s):              1.96
Input token throughput (tok/s):          2005.58
Output token throughput (tok/s):         2005.58
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   530116.82
Median E2E Latency (ms):                 528173.51
---------------Time to First Token----------------
Mean TTFT (ms):                          474135.60
Median TTFT (ms):                        482256.59
P99 TTFT (ms):                           954403.64
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          54.72
Median TPOT (ms):                        45.02
P99 TPOT (ms):                           84.30
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.72
Median ITL (ms):                         35.15
P99 ITL (ms):                            541.88
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 134.27s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:08:05<00:00,  8.17s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  4085.37
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2996363
Request throughput (req/s):              0.12
Input token throughput (tok/s):          244.78
Output token throughput (tok/s):         734.33
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2125347.84
Median E2E Latency (ms):                 2156193.17
---------------Time to First Token----------------
Mean TTFT (ms):                          1766464.74
Median TTFT (ms):                        1770942.42
P99 TTFT (ms):                           3817835.98
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          59.82
Median TPOT (ms):                        60.44
P99 TPOT (ms):                           105.87
---------------Inter-token Latency----------------
Mean ITL (ms):                           59.95
Median ITL (ms):                         36.54
P99 ITL (ms):                            38.56
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 45.57s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [35:20<00:00,  4.24s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2120.95
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000428
Request throughput (req/s):              0.24
Input token throughput (tok/s):          1414.46
Output token throughput (tok/s):         471.49
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1091990.30
Median E2E Latency (ms):                 1119952.10
---------------Time to First Token----------------
Mean TTFT (ms):                          996211.87
Median TTFT (ms):                        978785.22
P99 TTFT (ms):                           1993172.40
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.91
Median TPOT (ms):                        42.16
P99 TPOT (ms):                           79.75
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.92
Median ITL (ms):                         36.70
P99 ITL (ms):                            38.48
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-30B-A3B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-30B-A3B-FP8 --tp 1 --enable-metrics
```

```{text}
$ bash benchmark_serving.sh
Using backend: lmdeploy
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (1.26.4)
Requirement already satisfied: transformers in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (4.55.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: async-timeout<6.0,>=4.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (6.6.3)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: typing-extensions>=4.1.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp) (4.14.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)
Requirement already satisfied: filelock in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (2025.7.33)
Requirement already satisfied: requests in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from requests->transformers) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/lmdeploy/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 2.07s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:16<00:00, 73.40it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  136.24
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2002087
Request throughput (req/s):              73.40
Input token throughput (tok/s):          16578.12
Output token throughput (tok/s):         14622.68
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   63668.86
Median E2E Latency (ms):                 62881.95
---------------Time to First Token----------------
Mean TTFT (ms):                          53666.86
Median TTFT (ms):                        51653.97
P99 TTFT (ms):                           109041.43
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          52.01
Median TPOT (ms):                        56.59
P99 TPOT (ms):                           101.59
---------------Inter-token Latency----------------
Mean ITL (ms):                           113.47
Median ITL (ms):                         113.33
P99 ITL (ms):                            421.15
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 8.62s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:03<00:00,  6.59it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  303.60
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2049943
Request throughput (req/s):              6.59
Input token throughput (tok/s):          6745.70
Output token throughput (tok/s):         6745.70
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   167503.74
Median E2E Latency (ms):                 163138.67
---------------Time to First Token----------------
Mean TTFT (ms):                          121508.00
Median TTFT (ms):                        123844.29
P99 TTFT (ms):                           245794.24
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          44.96
Median TPOT (ms):                        38.45
P99 TPOT (ms):                           72.34
---------------Inter-token Latency----------------
Mean ITL (ms):                           45.67
Median ITL (ms):                         34.00
P99 ITL (ms):                            126.46
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 51.19s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [18:44<00:00,  2.25s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1124.50
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    3000285
Request throughput (req/s):              0.44
Input token throughput (tok/s):          889.28
Output token throughput (tok/s):         2667.85
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   632652.97
Median E2E Latency (ms):                 644839.88
---------------Time to First Token----------------
Mean TTFT (ms):                          344926.78
Median TTFT (ms):                        340974.99
P99 TTFT (ms):                           860987.45
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.96
Median TPOT (ms):                        46.93
P99 TPOT (ms):                           83.61
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.05
Median ITL (ms):                         29.75
P99 ITL (ms):                            109.90
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 17.39s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [09:04<00:00,  1.09s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  544.75
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000490
Request throughput (req/s):              0.92
Input token throughput (tok/s):          5507.13
Output token throughput (tok/s):         1835.71
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   299171.91
Median E2E Latency (ms):                 322588.83
---------------Time to First Token----------------
Mean TTFT (ms):                          231458.43
Median TTFT (ms):                        250365.89
P99 TTFT (ms):                           514199.64
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          33.87
Median TPOT (ms):                        31.34
P99 TPOT (ms):                           59.39
---------------Inter-token Latency----------------
Mean ITL (ms):                           33.93
Median ITL (ms):                         27.97
P99 ITL (ms):                            179.12
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-235B-A22B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-235B-A22B-FP8 --tp 4 --enable-metrics --communicator cuda-ipc
```

```{text}
$ bash benchmark_serving.sh
Using backend: lmdeploy
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 10.07s
100%|██████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:50<00:00, 34.38it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  290.84
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2001581
Request throughput (req/s):              34.38
Input token throughput (tok/s):          7765.64
Output token throughput (tok/s):         6849.66
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   131298.52
Median E2E Latency (ms):                 130922.27
---------------Time to First Token----------------
Mean TTFT (ms):                          108887.18
Median TTFT (ms):                        106809.45
P99 TTFT (ms):                           229571.45
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          115.94
Median TPOT (ms):                        120.30
P99 TPOT (ms):                           185.39
---------------Inter-token Latency----------------
Mean ITL (ms):                           129.32
Median ITL (ms):                         115.23
P99 ITL (ms):                            439.31
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 33.35s
100%|████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:15<00:00,  3.60it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  555.55
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2049468
Request throughput (req/s):              3.60
Input token throughput (tok/s):          3686.47
Output token throughput (tok/s):         3686.47
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   310489.72
Median E2E Latency (ms):                 303664.22
---------------Time to First Token----------------
Mean TTFT (ms):                          232305.67
Median TTFT (ms):                        216483.16
P99 TTFT (ms):                           494868.57
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          76.43
Median TPOT (ms):                        65.77
P99 TPOT (ms):                           123.43
---------------Inter-token Latency----------------
Mean ITL (ms):                           76.51
Median ITL (ms):                         53.76
P99 ITL (ms):                            361.28
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 197.41s
100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [28:24<00:00,  3.41s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1704.16
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999825
Request throughput (req/s):              0.29
Input token throughput (tok/s):          586.80
Output token throughput (tok/s):         1760.40
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   979079.75
Median E2E Latency (ms):                 978735.55
---------------Time to First Token----------------
Mean TTFT (ms):                          564554.16
Median TTFT (ms):                        712185.50
P99 TTFT (ms):                           1482237.05
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          69.10
Median TPOT (ms):                        69.62
P99 TPOT (ms):                           120.97
---------------Inter-token Latency----------------
Mean ITL (ms):                           69.17
Median ITL (ms):                         39.57
P99 ITL (ms):                            279.40
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 65.40s
100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [13:40<00:00,  1.64s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  820.31
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000313
Request throughput (req/s):              0.61
Input token throughput (tok/s):          3657.13
Output token throughput (tok/s):         1219.04
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   462126.82
Median E2E Latency (ms):                 468368.07
---------------Time to First Token----------------
Mean TTFT (ms):                          366951.70
Median TTFT (ms):                        377016.99
P99 TTFT (ms):                           747673.09
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.61
Median TPOT (ms):                        44.72
P99 TPOT (ms):                           82.27
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.69
Median ITL (ms):                         37.07
P99 ITL (ms):                            410.58
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### gpt-oss-20b

```shell
lmdeploy serve api_server openai/gpt-oss-20b --tp 1 --enable-metrics
```

```{text}
$ bash benchmark_serving.sh
Using backend: lmdeploy
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2229766
#Output tokens: 1959361
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.16s
100%|██████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:18<00:00, 72.20it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  138.50
Total input tokens:                      2229766
Total generated tokens:                  1959361
Total generated tokens (retokenized):    1955068
Request throughput (req/s):              72.20
Input token throughput (tok/s):          16099.67
Output token throughput (tok/s):         14147.26
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   70097.44
Median E2E Latency (ms):                 69854.74
---------------Time to First Token----------------
Mean TTFT (ms):                          59025.05
Median TTFT (ms):                        57336.24
P99 TTFT (ms):                           117878.63
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.01
Median TPOT (ms):                        63.34
P99 TPOT (ms):                           165.22
---------------Inter-token Latency----------------
Mean ITL (ms):                           112.11
Median ITL (ms):                         109.91
P99 ITL (ms):                            431.04
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 4.72s
100%|████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:51<00:00, 11.64it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  171.77
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1915409
Request throughput (req/s):              11.64
Input token throughput (tok/s):          11923.19
Output token throughput (tok/s):         11923.19
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   119832.64
Median E2E Latency (ms):                 130405.09
---------------Time to First Token----------------
Mean TTFT (ms):                          58404.16
Median TTFT (ms):                        21553.29
P99 TTFT (ms):                           138761.51
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.05
Median TPOT (ms):                        57.49
P99 TPOT (ms):                           106.48
---------------Inter-token Latency----------------
Mean ITL (ms):                           105.89
Median ITL (ms):                         80.46
P99 ITL (ms):                            275.68
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 27.72s
100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [06:24<00:00,  1.30it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  384.23
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2709411
Request throughput (req/s):              1.30
Input token throughput (tok/s):          2602.61
Output token throughput (tok/s):         7807.82
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   268972.32
Median E2E Latency (ms):                 280059.49
---------------Time to First Token----------------
Mean TTFT (ms):                          10074.84
Median TTFT (ms):                        9741.94
P99 TTFT (ms):                           19815.20
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          43.16
Median TPOT (ms):                        45.06
P99 TPOT (ms):                           60.68
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.78
Median ITL (ms):                         25.29
P99 ITL (ms):                            182.39
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 9.46s
100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:30<00:00,  2.38it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  210.26
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    802223
Request throughput (req/s):              2.38
Input token throughput (tok/s):          14267.87
Output token throughput (tok/s):         4755.96
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   138595.45
Median E2E Latency (ms):                 139703.25
---------------Time to First Token----------------
Mean TTFT (ms):                          75683.40
Median TTFT (ms):                        77540.42
P99 TTFT (ms):                           152619.83
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          31.47
Median TPOT (ms):                        31.09
P99 TPOT (ms):                           52.98
---------------Inter-token Latency----------------
Mean ITL (ms):                           35.29
Median ITL (ms):                         22.60
P99 ITL (ms):                            194.07
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### gpt-oss-120b

```shell
lmdeploy serve api_server openai/gpt-oss-20b --tp 2 --enable-metrics
```

```{text}
$ bash benchmark_serving.sh
Using backend: lmdeploy
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2229766
#Output tokens: 1959361
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.45s
100%|██████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:54<00:00, 57.32it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  174.47
Total input tokens:                      2229766
Total generated tokens:                  1959361
Total generated tokens (retokenized):    1928334
Request throughput (req/s):              57.32
Input token throughput (tok/s):          12780.52
Output token throughput (tok/s):         11230.62
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   85032.93
Median E2E Latency (ms):                 84620.87
---------------Time to First Token----------------
Mean TTFT (ms):                          70674.94
Median TTFT (ms):                        68610.75
P99 TTFT (ms):                           145861.78
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          76.11
Median TPOT (ms):                        81.02
P99 TPOT (ms):                           154.95
---------------Inter-token Latency----------------
Mean ITL (ms):                           108.78
Median ITL (ms):                         102.14
P99 ITL (ms):                            429.24
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.13s
100%|████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:25<00:00,  9.75it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  205.17
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1995406
Request throughput (req/s):              9.75
Input token throughput (tok/s):          9981.80
Output token throughput (tok/s):         9981.80
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   141496.26
Median E2E Latency (ms):                 153680.38
---------------Time to First Token----------------
Mean TTFT (ms):                          68718.97
Median TTFT (ms):                        82700.71
P99 TTFT (ms):                           161401.07
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          71.14
Median TPOT (ms):                        67.24
P99 TPOT (ms):                           126.54
---------------Inter-token Latency----------------
Mean ITL (ms):                           100.89
Median ITL (ms):                         81.59
P99 ITL (ms):                            278.86
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 35.60s
100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:55<00:00,  1.05it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  475.74
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2886811
Request throughput (req/s):              1.05
Input token throughput (tok/s):          2102.00
Output token throughput (tok/s):         6306.01
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   329960.83
Median E2E Latency (ms):                 338793.84
---------------Time to First Token----------------
Mean TTFT (ms):                          10474.87
Median TTFT (ms):                        10193.87
P99 TTFT (ms):                           20519.97
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          53.26
Median TPOT (ms):                        54.78
P99 TPOT (ms):                           75.80
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.58
Median ITL (ms):                         28.68
P99 ITL (ms):                            186.99
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 12.08s
100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [04:13<00:00,  1.97it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  253.56
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    977653
Request throughput (req/s):              1.97
Input token throughput (tok/s):          11831.34
Output token throughput (tok/s):         3943.78
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   162661.29
Median E2E Latency (ms):                 160599.40
---------------Time to First Token----------------
Mean TTFT (ms):                          88450.50
Median TTFT (ms):                        88531.79
P99 TTFT (ms):                           175271.21
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          37.12
Median TPOT (ms):                        36.05
P99 TPOT (ms):                           62.64
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.20
Median ITL (ms):                         26.39
P99 ITL (ms):                            196.93
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```
