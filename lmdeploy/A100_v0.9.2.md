# LMDeploy v0.9.2

```shell
python3 -m pip install lmdeploy==0.9.2
```

<!-- toc -->

## Table of Contents

- [Turbomind Engine](#turbomind-engine)

  - [Qwen3-8B](#qwen3-8b)
  - [Qwen3-32B](#qwen3-32b)
  - [Qwen3-30B-A3B](#qwen3-30b-a3b)
  - [Qwen3-235B-A22B](#qwen3-235b-a22b)

- [Pytorch Engine](#pytorch-engine)

  - [Qwen3-8B](#qwen3-8b-1)
  - [Qwen3-32B](#qwen3-32b-1)
  - [Qwen3-30B-A3B](#qwen3-30b-a3b-1)
  - [Qwen3-235B-A22B](#qwen3-235b-a22b-1)

<!-- tocstop -->

## Turbomind Engine

### Qwen3-8B

```shell
lmdeploy serve api_server Qwen/Qwen3-8B
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_by_lmdeploy_script/benchmark_serving.sh
Using backend: lmdeploy
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 2.87s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [07:00<00:00, 23.79it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  420.35
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2002120
Request throughput (req/s):              23.79
Input token throughput (tok/s):          5373.08
Output token throughput (tok/s):         4739.31
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   211229.01
Median E2E Latency (ms):                 210740.90
---------------Time to First Token----------------
Mean TTFT (ms):                          201255.22
Median TTFT (ms):                        200704.80
P99 TTFT (ms):                           403652.72
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          51.81
Median TPOT (ms):                        52.03
P99 TPOT (ms):                           81.14
---------------Inter-token Latency----------------
Mean ITL (ms):                           58.59
Median ITL (ms):                         51.10
P99 ITL (ms):                            156.38
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 12.19s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [10:34<00:00,  3.15it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  634.58
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2048475
Request throughput (req/s):              3.15
Input token throughput (tok/s):          3227.35
Output token throughput (tok/s):         3227.35
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   355831.78
Median E2E Latency (ms):                 362600.99
---------------Time to First Token----------------
Mean TTFT (ms):                          286288.93
Median TTFT (ms):                        295621.34
P99 TTFT (ms):                           588993.64
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          67.98
Median TPOT (ms):                        65.72
P99 TPOT (ms):                           103.99
---------------Inter-token Latency----------------
Mean ITL (ms):                           68.59
Median ITL (ms):                         50.35
P99 ITL (ms):                            528.78
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 74.11s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [33:42<00:00,  4.04s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2022.01
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998319
Request throughput (req/s):              0.25
Input token throughput (tok/s):          494.56
Output token throughput (tok/s):         1483.67
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1136634.78
Median E2E Latency (ms):                 1076195.50
---------------Time to First Token----------------
Mean TTFT (ms):                          687400.31
Median TTFT (ms):                        782157.94
P99 TTFT (ms):                           1642979.85
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          74.88
Median TPOT (ms):                        74.64
P99 TPOT (ms):                           130.00
---------------Inter-token Latency----------------
Mean ITL (ms):                           75.00
Median ITL (ms):                         42.78
P99 ITL (ms):                            440.25
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 25.56s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [17:51<00:00,  2.14s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1071.77
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000492
Request throughput (req/s):              0.47
Input token throughput (tok/s):          2799.10
Output token throughput (tok/s):         933.03
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   595678.74
Median E2E Latency (ms):                 569630.06
---------------Time to First Token----------------
Mean TTFT (ms):                          481545.88
Median TTFT (ms):                        462293.89
P99 TTFT (ms):                           1008867.61
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          57.09
Median TPOT (ms):                        53.74
P99 TPOT (ms):                           96.75
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.12
Median ITL (ms):                         42.13
P99 ITL (ms):                            588.66
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-32B

```shell
lmdeploy serve api_server Qwen/Qwen3-32B --tp 2
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_by_lmdeploy_script/benchmark_serving.sh
Using backend: lmdeploy
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.32s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [14:34<00:00, 11.43it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  874.97
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2000560
Request throughput (req/s):              11.43
Input token throughput (tok/s):          2581.34
Output token throughput (tok/s):         2276.86
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   436100.98
Median E2E Latency (ms):                 434837.24
---------------Time to First Token----------------
Mean TTFT (ms):                          414909.45
Median TTFT (ms):                        413916.44
P99 TTFT (ms):                           836741.43
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          110.25
Median TPOT (ms):                        109.19
P99 TPOT (ms):                           170.74
---------------Inter-token Latency----------------
Mean ITL (ms):                           107.45
Median ITL (ms):                         80.35
P99 ITL (ms):                            285.96
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 26.80s
██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [21:04<00:00,  1.58it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1264.44
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2049794
Request throughput (req/s):              1.58
Input token throughput (tok/s):          1619.69
Output token throughput (tok/s):         1619.69
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   693814.44
Median E2E Latency (ms):                 715406.50
---------------Time to First Token----------------
Mean TTFT (ms):                          562386.54
Median TTFT (ms):                        527437.11
P99 TTFT (ms):                           1147385.03
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          128.47
Median TPOT (ms):                        113.41
P99 TPOT (ms):                           198.90
---------------Inter-token Latency----------------
Mean ITL (ms):                           128.41
Median ITL (ms):                         73.87
P99 ITL (ms):                            1207.75
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 159.81s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [52:23<00:00,  6.29s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3143.91
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2995309
Request throughput (req/s):              0.16
Input token throughput (tok/s):          318.08
Output token throughput (tok/s):         954.23
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1745733.43
Median E2E Latency (ms):                 1774166.19
---------------Time to First Token----------------
Mean TTFT (ms):                          1158302.95
Median TTFT (ms):                        1388848.06
P99 TTFT (ms):                           2868762.98
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          97.92
Median TPOT (ms):                        96.83
P99 TPOT (ms):                           171.22
---------------Inter-token Latency----------------
Mean ITL (ms):                           98.13
Median ITL (ms):                         52.67
P99 ITL (ms):                            111.26
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 54.51s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [29:38<00:00,  3.56s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1778.01
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000432
Request throughput (req/s):              0.28
Input token throughput (tok/s):          1687.28
Output token throughput (tok/s):         562.43
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   966930.33
Median E2E Latency (ms):                 948112.50
---------------Time to First Token----------------
Mean TTFT (ms):                          808773.03
Median TTFT (ms):                        802502.10
P99 TTFT (ms):                           1700571.89
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          79.12
Median TPOT (ms):                        73.59
P99 TPOT (ms):                           129.72
---------------Inter-token Latency----------------
Mean ITL (ms):                           79.14
Median ITL (ms):                         51.67
P99 ITL (ms):                            1305.02
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-30B-A3B

```shell
lmdeploy serve api_server Qwen/Qwen3-30B-A3B --tp 2
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_by_lmdeploy_script/benchmark_serving.sh
Using backend: lmdeploy
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.94s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [05:50<00:00, 28.52it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  350.68
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2002101
Request throughput (req/s):              28.52
Input token throughput (tok/s):          6440.64
Output token throughput (tok/s):         5680.95
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   173912.41
Median E2E Latency (ms):                 172948.13
---------------Time to First Token----------------
Mean TTFT (ms):                          165688.02
Median TTFT (ms):                        164935.42
P99 TTFT (ms):                           332230.48
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          42.24
Median TPOT (ms):                        42.85
P99 TPOT (ms):                           55.19
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.14
Median ITL (ms):                         39.33
P99 ITL (ms):                            76.70
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 8.08s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:39<00:00,  5.01it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  399.45
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2049874
Request throughput (req/s):              5.01
Input token throughput (tok/s):          5127.07
Output token throughput (tok/s):         5127.07
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   225636.32
Median E2E Latency (ms):                 208047.09
---------------Time to First Token----------------
Mean TTFT (ms):                          176756.93
Median TTFT (ms):                        158058.23
P99 TTFT (ms):                           358085.48
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.78
Median TPOT (ms):                        48.73
P99 TPOT (ms):                           48.96
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.84
Median ITL (ms):                         44.10
P99 ITL (ms):                            178.71
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 48.65s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [17:01<00:00,  2.04s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1021.85
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    3000242
Request throughput (req/s):              0.49
Input token throughput (tok/s):          978.61
Output token throughput (tok/s):         2935.84
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   636536.78
Median E2E Latency (ms):                 592157.96
---------------Time to First Token----------------
Mean TTFT (ms):                          245317.53
Median TTFT (ms):                        11582.60
P99 TTFT (ms):                           595569.89
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          65.21
Median TPOT (ms):                        55.70
P99 TPOT (ms):                           96.85
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.44
Median ITL (ms):                         50.91
P99 ITL (ms):                            197.82
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 16.71s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:16<00:00,  1.01it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  496.88
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000474
Request throughput (req/s):              1.01
Input token throughput (tok/s):          6037.69
Output token throughput (tok/s):         2012.56
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   304826.75
Median E2E Latency (ms):                 336379.77
---------------Time to First Token----------------
Mean TTFT (ms):                          183698.07
Median TTFT (ms):                        217593.86
P99 TTFT (ms):                           377567.02
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.59
Median TPOT (ms):                        57.76
P99 TPOT (ms):                           103.90
---------------Inter-token Latency----------------
Mean ITL (ms):                           60.69
Median ITL (ms):                         48.34
P99 ITL (ms):                            241.09
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-235B-A22B

```shell
lmdeploy serve api_server Qwen/Qwen3-235B-A22B --tp 8
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_by_lmdeploy_script/benchmark_serving.sh
Using backend: lmdeploy
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 19.59s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [13:26<00:00, 12.40it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  806.54
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2001652
Request throughput (req/s):              12.40
Input token throughput (tok/s):          2800.33
Output token throughput (tok/s):         2470.02
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   381831.76
Median E2E Latency (ms):                 378689.06
---------------Time to First Token----------------
Mean TTFT (ms):                          363004.88
Median TTFT (ms):                        361158.10
P99 TTFT (ms):                           734043.26
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          96.85
Median TPOT (ms):                        96.06
P99 TPOT (ms):                           124.96
---------------Inter-token Latency----------------
Mean ITL (ms):                           95.68
Median ITL (ms):                         90.90
P99 ITL (ms):                            148.99
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 81.36s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [15:33<00:00,  2.14it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  933.83
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2046824
Request throughput (req/s):              2.14
Input token throughput (tok/s):          2193.12
Output token throughput (tok/s):         2193.12
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   489870.44
Median E2E Latency (ms):                 524964.23
---------------Time to First Token----------------
Mean TTFT (ms):                          384669.62
Median TTFT (ms):                        423063.84
P99 TTFT (ms):                           848520.08
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          102.84
Median TPOT (ms):                        99.41
P99 TPOT (ms):                           137.52
---------------Inter-token Latency----------------
Mean ITL (ms):                           102.82
Median ITL (ms):                         86.35
P99 ITL (ms):                            463.58
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 483.07s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [55:46<00:00,  6.69s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3346.42
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999312
Request throughput (req/s):              0.15
Input token throughput (tok/s):          298.83
Output token throughput (tok/s):         896.48
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1828394.57
Median E2E Latency (ms):                 1847106.67
---------------Time to First Token----------------
Mean TTFT (ms):                          1012894.08
Median TTFT (ms):                        961930.60
P99 TTFT (ms):                           2428495.18
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          135.94
Median TPOT (ms):                        136.85
P99 TPOT (ms):                           233.85
---------------Inter-token Latency----------------
Mean ITL (ms):                           136.09
Median ITL (ms):                         81.21
P99 ITL (ms):                            398.73
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 158.38s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [27:40<00:00,  3.32s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1660.81
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    998456
Request throughput (req/s):              0.30
Input token throughput (tok/s):          1806.35
Output token throughput (tok/s):         602.12
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   892681.51
Median E2E Latency (ms):                 936639.65
---------------Time to First Token----------------
Mean TTFT (ms):                          694195.92
Median TTFT (ms):                        755888.25
P99 TTFT (ms):                           1499545.14
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          99.29
Median TPOT (ms):                        91.74
P99 TPOT (ms):                           172.08
---------------Inter-token Latency----------------
Mean ITL (ms):                           99.34
Median ITL (ms):                         80.19
P99 ITL (ms):                            503.97
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

## Pytorch Engine

### Qwen3-8B

### Qwen3-32B

### Qwen3-30B-A3B

### Qwen3-235B-A22B
