# LMDeploy v0.9.2

```shell
python3 -m pip install lmdeploy==0.9.2
```

<!-- toc -->

## Table of Contents

- [Turbomind Engine](#turbomind-engine)

  - [Qwen3-8B-FP8](#qwen3-8b-fp8)
  - [Qwen3-32B-FP8](#qwen3-32b-fp8)
  - [Qwen3-30B-A3B-FP8](#qwen3-30b-a3b-fp8)
  - [Qwen3-235B-A22B-FP8](#qwen3-235b-a22b-fp8)

- [Pytorch Engine](#pytorch-engine)

  - [Qwen3-8B-FP8](#qwen3-8b-fp8-1)
  - [Qwen3-32B-FP8](#qwen3-32b-fp8-1)
  - [Qwen3-30B-A3B-FP8](#qwen3-30b-a3b-fp8-1)
  - [Qwen3-235B-A22B-FP8](#qwen3-235b-a22b-fp8-1)

<!-- tocstop -->

## TurboMind Engine

### Qwen3-8B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-8B-FP8 --max-batch-size 1024
```

```{text}
Using backend: lmdeploy
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.70s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:20<00:00, 71.05it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  140.75
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2002085
Request throughput (req/s):              71.05
Input token throughput (tok/s):          16047.05
Output token throughput (tok/s):         14154.26
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   68218.13
Median E2E Latency (ms):                 67558.95
---------------Time to First Token----------------
Mean TTFT (ms):                          57074.23
Median TTFT (ms):                        55114.71
P99 TTFT (ms):                           119696.67
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          57.19
Median TPOT (ms):                        63.58
P99 TPOT (ms):                           112.24
---------------Inter-token Latency----------------
Mean ITL (ms):                           103.78
Median ITL (ms):                         98.40
P99 ITL (ms):                            486.11
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 7.00s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:52<00:00,  6.84it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  292.49
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2048991
Request throughput (req/s):              6.84
Input token throughput (tok/s):          7002.02
Output token throughput (tok/s):         7002.02
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   166722.55
Median E2E Latency (ms):                 155076.32
---------------Time to First Token----------------
Mean TTFT (ms):                          124577.76
Median TTFT (ms):                        118036.54
P99 TTFT (ms):                           267681.75
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          41.20
Median TPOT (ms):                        36.04
P99 TPOT (ms):                           67.36
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.39
Median ITL (ms):                         30.17
P99 ITL (ms):                            167.08
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 42.89s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [17:32<00:00,  2.10s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1052.36
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998286
Request throughput (req/s):              0.48
Input token throughput (tok/s):          950.24
Output token throughput (tok/s):         2850.73
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   599132.27
Median E2E Latency (ms):                 600148.09
---------------Time to First Token----------------
Mean TTFT (ms):                          339781.44
Median TTFT (ms):                        310233.92
P99 TTFT (ms):                           774139.19
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          43.23
Median TPOT (ms):                        43.47
P99 TPOT (ms):                           74.84
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.31
Median ITL (ms):                         26.23
P99 ITL (ms):                            129.42
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 14.69s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:52<00:00,  1.06s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  532.14
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999400
Request throughput (req/s):              0.94
Input token throughput (tok/s):          5637.61
Output token throughput (tok/s):         1879.20
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   297328.38
Median E2E Latency (ms):                 303807.44
---------------Time to First Token----------------
Mean TTFT (ms):                          234435.07
Median TTFT (ms):                        244590.92
P99 TTFT (ms):                           487837.60
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          31.46
Median TPOT (ms):                        29.54
P99 TPOT (ms):                           55.43
---------------Inter-token Latency----------------
Mean ITL (ms):                           31.50
Median ITL (ms):                         25.55
P99 ITL (ms):                            205.87
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-32B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-32B-FP8 --max-batch-size 1024
```

```{text}
Using backend: lmdeploy
#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 5.22s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [07:58<00:00, 20.91it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  478.34
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2001312
Request throughput (req/s):              20.91
Input token throughput (tok/s):          4721.74
Output token throughput (tok/s):         4164.80
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   233501.22
Median E2E Latency (ms):                 232841.09
---------------Time to First Token----------------
Mean TTFT (ms):                          216887.26
Median TTFT (ms):                        214365.32
P99 TTFT (ms):                           446588.79
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          89.54
Median TPOT (ms):                        85.59
P99 TPOT (ms):                           158.29
---------------Inter-token Latency----------------
Mean ITL (ms):                           84.40
Median ITL (ms):                         74.40
P99 ITL (ms):                            219.83
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 22.18s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [16:56<00:00,  1.97it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1016.54
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2049043
Request throughput (req/s):              1.97
Input token throughput (tok/s):          2014.68
Output token throughput (tok/s):         2014.68
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   528493.82
Median E2E Latency (ms):                 526555.27
---------------Time to First Token----------------
Mean TTFT (ms):                          472596.56
Median TTFT (ms):                        480841.98
P99 TTFT (ms):                           951314.51
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          54.64
Median TPOT (ms):                        44.85
P99 TPOT (ms):                           83.97
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.63
Median ITL (ms):                         34.83
P99 ITL (ms):                            561.13
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 131.39s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:05:47<00:00,  7.89s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3947.15
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2995698
Request throughput (req/s):              0.13
Input token throughput (tok/s):          253.35
Output token throughput (tok/s):         760.04
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2048981.57
Median E2E Latency (ms):                 2074491.65
---------------Time to First Token----------------
Mean TTFT (ms):                          1700264.69
Median TTFT (ms):                        1705797.61
P99 TTFT (ms):                           3670423.97
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          58.13
Median TPOT (ms):                        58.34
P99 TPOT (ms):                           102.79
---------------Inter-token Latency----------------
Mean ITL (ms):                           58.24
Median ITL (ms):                         35.07
P99 ITL (ms):                            37.23
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 44.69s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [34:22<00:00,  4.12s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2062.33
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000492
Request throughput (req/s):              0.24
Input token throughput (tok/s):          1454.67
Output token throughput (tok/s):         484.89
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1061712.65
Median E2E Latency (ms):                 1088893.82
---------------Time to First Token----------------
Mean TTFT (ms):                          968648.31
Median TTFT (ms):                        951764.70
P99 TTFT (ms):                           1938106.35
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          46.56
Median TPOT (ms):                        40.96
P99 TPOT (ms):                           77.45
---------------Inter-token Latency----------------
Mean ITL (ms):                           46.57
Median ITL (ms):                         35.79
P99 ITL (ms):                            37.54
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-30B-A3B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-30B-A3B-FP8 --max-batch-size 1024
```

```{text}
Using backend: lmdeploy

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.98s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:16<00:00, 73.37it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  136.29
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2002067
Request throughput (req/s):              73.37
Input token throughput (tok/s):          16571.87
Output token throughput (tok/s):         14617.17
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   64184.81
Median E2E Latency (ms):                 63572.26
---------------Time to First Token----------------
Mean TTFT (ms):                          54057.47
Median TTFT (ms):                        52280.11
P99 TTFT (ms):                           107459.02
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          53.41
Median TPOT (ms):                        57.01
P99 TPOT (ms):                           129.50
---------------Inter-token Latency----------------
Mean ITL (ms):                           101.22
Median ITL (ms):                         101.35
P99 ITL (ms):                            425.70
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 8.44s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:00<00:00,  6.65it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  300.77
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2049935
Request throughput (req/s):              6.65
Input token throughput (tok/s):          6809.20
Output token throughput (tok/s):         6809.20
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   165936.52
Median E2E Latency (ms):                 161715.05
---------------Time to First Token----------------
Mean TTFT (ms):                          120285.02
Median TTFT (ms):                        122631.40
P99 TTFT (ms):                           243839.12
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          44.63
Median TPOT (ms):                        38.15
P99 TPOT (ms):                           71.73
---------------Inter-token Latency----------------
Mean ITL (ms):                           45.15
Median ITL (ms):                         33.52
P99 ITL (ms):                            128.12
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 49.57s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [18:38<00:00,  2.24s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1118.69
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999605
Request throughput (req/s):              0.45
Input token throughput (tok/s):          893.90
Output token throughput (tok/s):         2681.70
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   629926.01
Median E2E Latency (ms):                 641962.55
---------------Time to First Token----------------
Mean TTFT (ms):                          343143.02
Median TTFT (ms):                        340511.04
P99 TTFT (ms):                           856486.35
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.81
Median TPOT (ms):                        46.78
P99 TPOT (ms):                           83.35
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.87
Median ITL (ms):                         29.58
P99 ITL (ms):                            110.70
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 16.92s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [09:00<00:00,  1.08s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  540.90
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000480
Request throughput (req/s):              0.92
Input token throughput (tok/s):          5546.29
Output token throughput (tok/s):         1848.76
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   297302.57
Median E2E Latency (ms):                 320212.23
---------------Time to First Token----------------
Mean TTFT (ms):                          229827.25
Median TTFT (ms):                        257412.08
P99 TTFT (ms):                           511573.34
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          33.75
Median TPOT (ms):                        31.29
P99 TPOT (ms):                           59.62
---------------Inter-token Latency----------------
Mean ITL (ms):                           33.80
Median ITL (ms):                         27.82
P99 ITL (ms):                            180.08
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

### Qwen3-235B-A22B-FP8

```shell
lmdeploy serve api_server Qwen/Qwen3-235B-A22B-FP8 --max-batch-size 1024 --tp 4
```

```{text}
Using backend: lmdeploy

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 7.31s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [05:03<00:00, 33.00it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  303.02
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    2001511
Request throughput (req/s):              33.00
Input token throughput (tok/s):          7453.60
Output token throughput (tok/s):         6574.43
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   136398.61
Median E2E Latency (ms):                 136057.86
---------------Time to First Token----------------
Mean TTFT (ms):                          112598.44
Median TTFT (ms):                        111040.36
P99 TTFT (ms):                           237472.28
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          124.08
Median TPOT (ms):                        127.71
P99 TPOT (ms):                           214.03
---------------Inter-token Latency----------------
Mean ITL (ms):                           132.06
Median ITL (ms):                         117.91
P99 ITL (ms):                            463.02
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 30.34s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:44<00:00,  3.42it/s]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  584.90
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2049762
Request throughput (req/s):              3.42
Input token throughput (tok/s):          3501.48
Output token throughput (tok/s):         3501.48
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   329258.63
Median E2E Latency (ms):                 309830.94
---------------Time to First Token----------------
Mean TTFT (ms):                          243942.47
Median TTFT (ms):                        236341.03
P99 TTFT (ms):                           534672.59
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          83.40
Median TPOT (ms):                        71.79
P99 TPOT (ms):                           133.28
---------------Inter-token Latency----------------
Mean ITL (ms):                           83.40
Median ITL (ms):                         59.25
P99 ITL (ms):                            376.31
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 176.10s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [28:54<00:00,  3.47s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1734.77
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    3000064
Request throughput (req/s):              0.29
Input token throughput (tok/s):          576.45
Output token throughput (tok/s):         1729.34
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   969176.95
Median E2E Latency (ms):                 975195.87
---------------Time to First Token----------------
Mean TTFT (ms):                          544142.31
Median TTFT (ms):                        508775.91
P99 TTFT (ms):                           1265761.58
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          70.85
Median TPOT (ms):                        71.23
P99 TPOT (ms):                           121.15
---------------Inter-token Latency----------------
Mean ITL (ms):                           70.92
Median ITL (ms):                         41.12
P99 ITL (ms):                            294.07
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: lmdeploy
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='lmdeploy', base_url=None, host='0.0.0.0', port=23333, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 59.77s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [14:15<00:00,  1.71s/it]

============ Serving Benchmark Result ============
Backend:                                 lmdeploy
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  855.68
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    1000464
Request throughput (req/s):              0.58
Input token throughput (tok/s):          3505.97
Output token throughput (tok/s):         1168.66
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   472017.24
Median E2E Latency (ms):                 486192.01
---------------Time to First Token----------------
Mean TTFT (ms):                          371207.32
Median TTFT (ms):                        391419.31
P99 TTFT (ms):                           787073.85
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          50.43
Median TPOT (ms):                        47.40
P99 TPOT (ms):                           86.92
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.51
Median ITL (ms):                         38.79
P99 ITL (ms):                            424.67
==================================================
----------------------------------------
All benchmarks completed for backend: lmdeploy
```

## Pytorch engine

### Qwen3-8B-FP8

### Qwen3-32B-FP8

### Qwen3-30B-A3B-FP8

### Qwen3-235B-A22B-FP8
