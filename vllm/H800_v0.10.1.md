# vLLM v0.10.1

Prepare the docker image:

```shell
docker pull vllm/vllm-openai:v0.10.1
```

Serve a model via docker:

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model ${MODEL_REPO_ID} --tensor-parallel-size ${TP} \
    --host 0.0.0.0
```

<!-- toc -->

## Table of Contents

- [vLLM v0.10.1](#vllm-v0101)
  - [Table of Contents](#table-of-contents)
  - [Qwen3-8B-FP8](#qwen3-8b-fp8)
  - [Qwen3-32B-FP8](#qwen3-32b-fp8)
  - [Qwen3-30B-A3B-FP8](#qwen3-30b-a3b-fp8)
  - [Qwen3-235B-A22B-FP8](#qwen3-235b-a22b-fp8)
  - [gpt-oss-20b](#gpt-oss-20b)
  - [gpt-oss-120b](#gpt-oss-120b)

<!-- tocstop -->

## Qwen3-8B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-8B-FP8 --tensor-parallel-size 1 \
    --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh vllm
Using backend: vllm
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.88s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:54<00:00, 57.42it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  174.16
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1992147
Request throughput (req/s):              57.42
Input token throughput (tok/s):          12968.34
Output token throughput (tok/s):         11438.69
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   86797.66
Median E2E Latency (ms):                 86558.17
---------------Time to First Token----------------
Mean TTFT (ms):                          71512.66
Median TTFT (ms):                        70259.68
P99 TTFT (ms):                           148438.42
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          84.00
Median TPOT (ms):                        80.74
P99 TPOT (ms):                           210.88
---------------Inter-token Latency----------------
Mean ITL (ms):                           77.17
Median ITL (ms):                         74.06
P99 ITL (ms):                            184.09
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 7.46s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:22<00:00,  6.19it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  322.91
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047982
Request throughput (req/s):              6.19
Input token throughput (tok/s):          6342.42
Output token throughput (tok/s):         6342.42
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   184766.76
Median E2E Latency (ms):                 185107.32
---------------Time to First Token----------------
Mean TTFT (ms):                          133143.99
Median TTFT (ms):                        141379.78
P99 TTFT (ms):                           279429.04
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          50.46
Median TPOT (ms):                        42.75
P99 TPOT (ms):                           80.77
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.49
Median ITL (ms):                         35.63
P99 ITL (ms):                            184.80
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 44.69s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [18:19<00:00,  2.20s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1099.10
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998016
Request throughput (req/s):              0.45
Input token throughput (tok/s):          909.84
Output token throughput (tok/s):         2729.51
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   626564.82
Median E2E Latency (ms):                 641474.58
---------------Time to First Token----------------
Mean TTFT (ms):                          339566.95
Median TTFT (ms):                        343360.86
P99 TTFT (ms):                           871528.14
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.84
Median TPOT (ms):                        47.36
P99 TPOT (ms):                           84.12
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.90
Median ITL (ms):                         29.33
P99 ITL (ms):                            142.90
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 15.23s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [09:17<00:00,  1.12s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  557.80
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    998997
Request throughput (req/s):              0.90
Input token throughput (tok/s):          5378.27
Output token throughput (tok/s):         1792.76
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   310524.54
Median E2E Latency (ms):                 324708.36
---------------Time to First Token----------------
Mean TTFT (ms):                          239258.05
Median TTFT (ms):                        214089.72
P99 TTFT (ms):                           485741.81
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          35.65
Median TPOT (ms):                        33.37
P99 TPOT (ms):                           61.95
---------------Inter-token Latency----------------
Mean ITL (ms):                           35.68
Median ITL (ms):                         28.88
P99 ITL (ms):                            201.92
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## Qwen3-32B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-32B-FP8 --tensor-parallel-size 1 \
    --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh vllm
Using backend: vllm
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 5.05s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:42<00:00, 19.16it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  522.04
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1990385
Request throughput (req/s):              19.16
Input token throughput (tok/s):          4326.47
Output token throughput (tok/s):         3816.15
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   259476.83
Median E2E Latency (ms):                 258575.90
---------------Time to First Token----------------
Mean TTFT (ms):                          239222.07
Median TTFT (ms):                        236659.64
P99 TTFT (ms):                           488458.59
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          118.82
Median TPOT (ms):                        101.25
P99 TPOT (ms):                           644.60
---------------Inter-token Latency----------------
Mean ITL (ms):                           102.31
Median ITL (ms):                         79.90
P99 ITL (ms):                            312.64
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 20.68s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [16:54<00:00,  1.97it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1014.18
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047811
Request throughput (req/s):              1.97
Input token throughput (tok/s):          2019.36
Output token throughput (tok/s):         2019.36
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   529593.30
Median E2E Latency (ms):                 538138.77
---------------Time to First Token----------------
Mean TTFT (ms):                          471775.52
Median TTFT (ms):                        453769.06
P99 TTFT (ms):                           945111.00
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          56.52
Median TPOT (ms):                        46.03
P99 TPOT (ms):                           85.81
---------------Inter-token Latency----------------
Mean ITL (ms):                           56.54
Median ITL (ms):                         34.53
P99 ITL (ms):                            680.07
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 122.62s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:00:42<00:00,  7.29s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3642.97
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998886
Request throughput (req/s):              0.14
Input token throughput (tok/s):          274.50
Output token throughput (tok/s):         823.50
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1904163.53
Median E2E Latency (ms):                 1907063.39
---------------Time to First Token----------------
Mean TTFT (ms):                          1577727.21
Median TTFT (ms):                        1611184.55
P99 TTFT (ms):                           3456571.42
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          54.42
Median TPOT (ms):                        56.74
P99 TPOT (ms):                           94.76
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.48
Median ITL (ms):                         33.20
P99 ITL (ms):                            36.07
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 41.76s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [32:03<00:00,  3.85s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1923.95
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999923
Request throughput (req/s):              0.26
Input token throughput (tok/s):          1559.29
Output token throughput (tok/s):         519.76
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1000558.52
Median E2E Latency (ms):                 981199.64
---------------Time to First Token----------------
Mean TTFT (ms):                          916090.08
Median TTFT (ms):                        902608.21
P99 TTFT (ms):                           1867726.54
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          42.26
Median TPOT (ms):                        39.32
P99 TPOT (ms):                           70.79
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.28
Median ITL (ms):                         33.38
P99 ITL (ms):                            38.03
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## Qwen3-30B-A3B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-30B-A3B-FP8 --tensor-parallel-size 1 \
    --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh vllm
Using backend: vllm
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.67s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:59<00:00, 55.59it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  179.90
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991953
Request throughput (req/s):              55.59
Input token throughput (tok/s):          12554.92
Output token throughput (tok/s):         11074.03
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   86503.24
Median E2E Latency (ms):                 85745.23
---------------Time to First Token----------------
Mean TTFT (ms):                          71146.39
Median TTFT (ms):                        70659.21
P99 TTFT (ms):                           145686.23
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          83.51
Median TPOT (ms):                        77.86
P99 TPOT (ms):                           185.35
---------------Inter-token Latency----------------
Mean ITL (ms):                           77.50
Median ITL (ms):                         72.26
P99 ITL (ms):                            157.40
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.86s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:28<00:00,  6.09it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  328.19
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047949
Request throughput (req/s):              6.09
Input token throughput (tok/s):          6240.37
Output token throughput (tok/s):         6240.37
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   184207.95
Median E2E Latency (ms):                 183858.31
---------------Time to First Token----------------
Mean TTFT (ms):                          131864.02
Median TTFT (ms):                        140266.30
P99 TTFT (ms):                           278407.77
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          51.17
Median TPOT (ms):                        43.21
P99 TPOT (ms):                           82.40
---------------Inter-token Latency----------------
Mean ITL (ms):                           51.19
Median ITL (ms):                         36.94
P99 ITL (ms):                            150.26
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 40.68s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [18:53<00:00,  2.27s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1133.93
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999798
Request throughput (req/s):              0.44
Input token throughput (tok/s):          881.89
Output token throughput (tok/s):         2645.67
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   641178.21
Median E2E Latency (ms):                 657379.66
---------------Time to First Token----------------
Mean TTFT (ms):                          345660.19
Median TTFT (ms):                        351241.21
P99 TTFT (ms):                           888252.77
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          49.26
Median TPOT (ms):                        48.78
P99 TPOT (ms):                           85.96
---------------Inter-token Latency----------------
Mean ITL (ms):                           49.33
Median ITL (ms):                         30.44
P99 ITL (ms):                            125.04
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 13.89s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [09:05<00:00,  1.09s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  545.86
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999968
Request throughput (req/s):              0.92
Input token throughput (tok/s):          5495.89
Output token throughput (tok/s):         1831.96
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   302913.51
Median E2E Latency (ms):                 321202.38
---------------Time to First Token----------------
Mean TTFT (ms):                          233041.23
Median TTFT (ms):                        207985.58
P99 TTFT (ms):                           471622.40
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          34.95
Median TPOT (ms):                        32.56
P99 TPOT (ms):                           60.59
---------------Inter-token Latency----------------
Mean ITL (ms):                           34.98
Median ITL (ms):                         28.82
P99 ITL (ms):                            171.84
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## Qwen3-235B-A22B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-235B-A22B-FP8 --tensor-parallel-size 4 \
    --host 0.0.0.0

```

```{text}
$ bash benchmark_serving.sh vllm
Using backend: vllm
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 3.57s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [06:44<00:00, 24.73it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  404.29
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991709
Request throughput (req/s):              24.73
Input token throughput (tok/s):          5586.57
Output token throughput (tok/s):         4927.62
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   193649.15
Median E2E Latency (ms):                 189603.56
---------------Time to First Token----------------
Mean TTFT (ms):                          166273.49
Median TTFT (ms):                        163804.00
P99 TTFT (ms):                           353910.46
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          153.28
Median TPOT (ms):                        138.59
P99 TPOT (ms):                           429.80
---------------Inter-token Latency----------------
Mean ITL (ms):                           138.30
Median ITL (ms):                         123.08
P99 ITL (ms):                            416.02
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 14.00s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [11:52<00:00,  2.81it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  712.84
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2045750
Request throughput (req/s):              2.81
Input token throughput (tok/s):          2873.02
Output token throughput (tok/s):         2873.02
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   382392.27
Median E2E Latency (ms):                 379056.60
---------------Time to First Token----------------
Mean TTFT (ms):                          309264.03
Median TTFT (ms):                        319362.26
P99 TTFT (ms):                           635098.92
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          71.48
Median TPOT (ms):                        59.02
P99 TPOT (ms):                           110.95
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.52
Median ITL (ms):                         46.69
P99 ITL (ms):                            390.60
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 82.08s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [34:23<00:00,  4.13s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2063.30
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998591
Request throughput (req/s):              0.24
Input token throughput (tok/s):          484.66
Output token throughput (tok/s):         1453.98
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1130622.58
Median E2E Latency (ms):                 1153242.62
---------------Time to First Token----------------
Mean TTFT (ms):                          789546.78
Median TTFT (ms):                        824910.56
P99 TTFT (ms):                           1882786.35
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          56.86
Median TPOT (ms):                        58.38
P99 TPOT (ms):                           101.23
---------------Inter-token Latency----------------
Mean ITL (ms):                           56.92
Median ITL (ms):                         34.51
P99 ITL (ms):                            45.01
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 27.90s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [16:47<00:00,  2.02s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1007.72
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999903
Request throughput (req/s):              0.50
Input token throughput (tok/s):          2977.03
Output token throughput (tok/s):         992.34
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   546979.88
Median E2E Latency (ms):                 557462.92
---------------Time to First Token----------------
Mean TTFT (ms):                          465421.79
Median TTFT (ms):                        480445.38
P99 TTFT (ms):                           951135.09
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          40.80
Median TPOT (ms):                        38.47
P99 TPOT (ms):                           70.28
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.85
Median ITL (ms):                         32.36
P99 ITL (ms):                            408.41
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## gpt-oss-20b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model openai/gpt-oss-20b --tensor-parallel-size 1 \
    --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh vllm
Using backend: vllm
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2229766
#Output tokens: 1959361
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.07s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [03:24<00:00, 48.81it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  204.87
Total input tokens:                      2229766
Total generated tokens:                  1959361
Total generated tokens (retokenized):    1943170
Request throughput (req/s):              48.81
Input token throughput (tok/s):          10883.61
Output token throughput (tok/s):         9563.75
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   105313.56
Median E2E Latency (ms):                 104909.04
---------------Time to First Token----------------
Mean TTFT (ms):                          86785.94
Median TTFT (ms):                        85251.84
P99 TTFT (ms):                           179886.76
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          104.23
Median TPOT (ms):                        99.15
P99 TPOT (ms):                           253.84
---------------Inter-token Latency----------------
Mean ITL (ms):                           95.13
Median ITL (ms):                         89.78
P99 ITL (ms):                            237.26
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 4.40s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:24<00:00,  9.79it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  204.23
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1911235
Request throughput (req/s):              9.79
Input token throughput (tok/s):          10028.07
Output token throughput (tok/s):         10028.07
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   158324.82
Median E2E Latency (ms):                 133815.75
---------------Time to First Token----------------
Mean TTFT (ms):                          66123.64
Median TTFT (ms):                        32898.53
P99 TTFT (ms):                           134674.36
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          90.13
Median TPOT (ms):                        96.31
P99 TPOT (ms):                           98.69
---------------Inter-token Latency----------------
Mean ITL (ms):                           90.37
Median ITL (ms):                         69.79
P99 ITL (ms):                            261.55
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 25.77s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [06:54<00:00,  1.21it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  414.47
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2733380
Request throughput (req/s):              1.21
Input token throughput (tok/s):          2412.71
Output token throughput (tok/s):         7238.12
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   347811.95
Median E2E Latency (ms):                 324417.41
---------------Time to First Token----------------
Mean TTFT (ms):                          14803.07
Median TTFT (ms):                        14767.05
P99 TTFT (ms):                           29022.27
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          55.51
Median TPOT (ms):                        51.62
P99 TPOT (ms):                           64.20
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.71
Median ITL (ms):                         46.21
P99 ITL (ms):                            231.94
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 8.84s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:58<00:00,  2.10it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  238.53
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    811139
Request throughput (req/s):              2.10
Input token throughput (tok/s):          12577.04
Output token throughput (tok/s):         4192.35
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   195471.24
Median E2E Latency (ms):                 188679.21
---------------Time to First Token----------------
Mean TTFT (ms):                          68490.34
Median TTFT (ms):                        45264.45
P99 TTFT (ms):                           186863.38
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          63.52
Median TPOT (ms):                        72.89
P99 TPOT (ms):                           83.06
---------------Inter-token Latency----------------
Mean ITL (ms):                           63.92
Median ITL (ms):                         43.46
P99 ITL (ms):                            252.69
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## gpt-oss-120b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model openai/gpt-oss-120b --tensor-parallel-size 2 \
    --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh vllm
Using backend: vllm
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2229766
#Output tokens: 1959361
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.30s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:03<00:00, 41.06it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  243.57
Total input tokens:                      2229766
Total generated tokens:                  1959361
Total generated tokens (retokenized):    1915343
Request throughput (req/s):              41.06
Input token throughput (tok/s):          9154.37
Output token throughput (tok/s):         8044.22
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   123457.42
Median E2E Latency (ms):                 123246.99
---------------Time to First Token----------------
Mean TTFT (ms):                          101461.16
Median TTFT (ms):                        100744.42
P99 TTFT (ms):                           212321.62
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          122.17
Median TPOT (ms):                        118.20
P99 TPOT (ms):                           255.37
---------------Inter-token Latency----------------
Mean ITL (ms):                           112.92
Median ITL (ms):                         108.30
P99 ITL (ms):                            569.88
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 5.36s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:09<00:00,  8.01it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  249.65
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1989060
Request throughput (req/s):              8.01
Input token throughput (tok/s):          8203.50
Output token throughput (tok/s):         8203.50
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   186396.39
Median E2E Latency (ms):                 173788.43
---------------Time to First Token----------------
Mean TTFT (ms):                          76654.25
Median TTFT (ms):                        34227.90
P99 TTFT (ms):                           162917.44
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          107.27
Median TPOT (ms):                        110.12
P99 TPOT (ms):                           136.92
---------------Inter-token Latency----------------
Mean ITL (ms):                           107.62
Median ITL (ms):                         82.28
P99 ITL (ms):                            519.05
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 31.30s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:08<00:00,  1.02it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  488.78
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2870795
Request throughput (req/s):              1.02
Input token throughput (tok/s):          2045.92
Output token throughput (tok/s):         6137.76
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   389932.49
Median E2E Latency (ms):                 372357.60
---------------Time to First Token----------------
Mean TTFT (ms):                          15233.01
Median TTFT (ms):                        15297.52
P99 TTFT (ms):                           30341.07
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          62.46
Median TPOT (ms):                        59.52
P99 TPOT (ms):                           76.36
---------------Inter-token Latency----------------
Mean ITL (ms):                           63.01
Median ITL (ms):                         46.95
P99 ITL (ms):                            243.28
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 10.68s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [04:18<00:00,  1.93it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  258.87
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    972956
Request throughput (req/s):              1.93
Input token throughput (tok/s):          11588.98
Output token throughput (tok/s):         3862.99
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   203125.82
Median E2E Latency (ms):                 196389.51
---------------Time to First Token----------------
Mean TTFT (ms):                          81500.65
Median TTFT (ms):                        46289.53
P99 TTFT (ms):                           182592.20
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.84
Median TPOT (ms):                        65.70
P99 TPOT (ms):                           96.70
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.11
Median ITL (ms):                         40.28
P99 ITL (ms):                            253.53
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```
