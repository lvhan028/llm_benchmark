# vLLM v0.10.1

Prepare the docker image:

```shell
docker pull vllm/vllm-openai:v0.10.1
```

Serve a model via docker:

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model ${MODEL_REPO_ID} --tensor-parallel-size ${TP} \
    --host 0.0.0.0
```

<!-- toc -->

## Table of Contents

- [vLLM v0.10.1](#vllm-v0101)
  - [Table of Contents](#table-of-contents)
  - [Qwen3-8B](#qwen3-8b)
  - [Qwen3-32B](#qwen3-32b)
  - [Qwen3-30B-A3B](#qwen3-30b-a3b)
  - [Qwen3-235B-A22B](#qwen3-235b-a22b)
  - [gpt-oss-20b](#gpt-oss-20b)
  - [gpt-oss-120b](#gpt-oss-120b)

<!-- tocstop -->

## Qwen3-8B

Serve a model via docker:

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-8B --tensor-parallel-size 1 \
    --host 0.0.0.0
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_serving.sh vllm
Using backend: vllm
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 2.96s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:38<00:00, 17.29it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  578.48
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1992117
Request throughput (req/s):              17.29
Input token throughput (tok/s):          3904.34
Output token throughput (tok/s):         3443.81
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   289213.70
Median E2E Latency (ms):                 288336.32
---------------Time to First Token----------------
Mean TTFT (ms):                          275009.37
Median TTFT (ms):                        274460.89
P99 TTFT (ms):                           555801.53
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          73.02
Median TPOT (ms):                        72.39
P99 TPOT (ms):                           118.02
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.71
Median ITL (ms):                         65.32
P99 ITL (ms):                            161.67
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 12.45s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [12:59<00:00,  2.57it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  779.02
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2046480
Request throughput (req/s):              2.57
Input token throughput (tok/s):          2628.95
Output token throughput (tok/s):         2628.95
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   434396.74
Median E2E Latency (ms):                 459512.65
---------------Time to First Token----------------
Mean TTFT (ms):                          347054.59
Median TTFT (ms):                        374471.24
P99 TTFT (ms):                           752993.18
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          85.38
Median TPOT (ms):                        84.43
P99 TPOT (ms):                           116.35
---------------Inter-token Latency----------------
Mean ITL (ms):                           85.41
Median ITL (ms):                         58.48
P99 ITL (ms):                            271.11
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 75.80s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [38:56<00:00,  4.67s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2336.34
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998146
Request throughput (req/s):              0.21
Input token throughput (tok/s):          428.02
Output token throughput (tok/s):         1284.06
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1336787.61
Median E2E Latency (ms):                 1349661.71
---------------Time to First Token----------------
Mean TTFT (ms):                          772474.43
Median TTFT (ms):                        707671.21
P99 TTFT (ms):                           1768755.83
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          94.07
Median TPOT (ms):                        93.43
P99 TPOT (ms):                           163.03
---------------Inter-token Latency----------------
Mean ITL (ms):                           94.22
Median ITL (ms):                         52.47
P99 ITL (ms):                            271.46
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 26.12s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [21:39<00:00,  2.60s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1299.32
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999972
Request throughput (req/s):              0.38
Input token throughput (tok/s):          2308.90
Output token throughput (tok/s):         769.63
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   732611.64
Median E2E Latency (ms):                 749162.18
---------------Time to First Token----------------
Mean TTFT (ms):                          584507.28
Median TTFT (ms):                        605799.34
P99 TTFT (ms):                           1217166.83
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          74.09
Median TPOT (ms):                        71.82
P99 TPOT (ms):                           122.34
---------------Inter-token Latency----------------
Mean ITL (ms):                           74.15
Median ITL (ms):                         52.23
P99 ITL (ms):                            272.64
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## Qwen3-32B

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-32B --tensor-parallel-size 2 \
    --host 0.0.0.0
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_serving.sh vllm
Using backend: vllm
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.02s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [19:27<00:00,  8.57it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  1167.33
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991584
Request throughput (req/s):              8.57
Input token throughput (tok/s):          1934.83
Output token throughput (tok/s):         1706.62
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   582979.39
Median E2E Latency (ms):                 581530.18
---------------Time to First Token----------------
Mean TTFT (ms):                          554124.28
Median TTFT (ms):                        553122.58
P99 TTFT (ms):                           1125438.96
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          148.28
Median TPOT (ms):                        147.30
P99 TPOT (ms):                           250.37
---------------Inter-token Latency----------------
Mean ITL (ms):                           145.75
Median ITL (ms):                         129.39
P99 ITL (ms):                            341.42
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 25.85s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [24:29<00:00,  1.36it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1469.76
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047834
Request throughput (req/s):              1.36
Input token throughput (tok/s):          1393.42
Output token throughput (tok/s):         1393.42
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   821177.57
Median E2E Latency (ms):                 786596.26
---------------Time to First Token----------------
Mean TTFT (ms):                          670357.78
Median TTFT (ms):                        649274.82
P99 TTFT (ms):                           1402883.08
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          147.43
Median TPOT (ms):                        137.83
P99 TPOT (ms):                           228.75
---------------Inter-token Latency----------------
Mean ITL (ms):                           147.50
Median ITL (ms):                         74.86
P99 ITL (ms):                            494.14
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 154.80s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [59:22<00:00,  7.12s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3562.12
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2995107
Request throughput (req/s):              0.14
Input token throughput (tok/s):          280.73
Output token throughput (tok/s):         842.20
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1967053.36
Median E2E Latency (ms):                 2012583.61
---------------Time to First Token----------------
Mean TTFT (ms):                          1296091.37
Median TTFT (ms):                        1243429.22
P99 TTFT (ms):                           2980184.88
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          111.85
Median TPOT (ms):                        113.49
P99 TPOT (ms):                           192.22
---------------Inter-token Latency----------------
Mean ITL (ms):                           112.10
Median ITL (ms):                         60.34
P99 ITL (ms):                            492.95
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 53.13s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [33:58<00:00,  4.08s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2038.76
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999938
Request throughput (req/s):              0.25
Input token throughput (tok/s):          1471.48
Output token throughput (tok/s):         490.49
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1110648.77
Median E2E Latency (ms):                 1108828.67
---------------Time to First Token----------------
Mean TTFT (ms):                          929596.48
Median TTFT (ms):                        935792.69
P99 TTFT (ms):                           1871999.72
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          90.57
Median TPOT (ms):                        86.54
P99 TPOT (ms):                           143.09
---------------Inter-token Latency----------------
Mean ITL (ms):                           90.64
Median ITL (ms):                         54.09
P99 ITL (ms):                            496.55
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## Qwen3-30B-A3B

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-30B-A3B --tensor-parallel-size 2 \
    --host 0.0.0.0
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_serving.sh vllm
Using backend: vllm
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.94s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:20<00:00, 19.97it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  500.69
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991987
Request throughput (req/s):              19.97
Input token throughput (tok/s):          4510.94
Output token throughput (tok/s):         3978.86
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   247380.91
Median E2E Latency (ms):                 246022.03
---------------Time to First Token----------------
Mean TTFT (ms):                          235173.12
Median TTFT (ms):                        234230.04
P99 TTFT (ms):                           475182.86
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          62.61
Median TPOT (ms):                        61.97
P99 TPOT (ms):                           98.05
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.62
Median ITL (ms):                         62.45
P99 ITL (ms):                            97.54
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 7.90s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:02<00:00,  3.69it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  542.49
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047929
Request throughput (req/s):              3.69
Input token throughput (tok/s):          3775.18
Output token throughput (tok/s):         3775.18
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   308063.72
Median E2E Latency (ms):                 291167.89
---------------Time to First Token----------------
Mean TTFT (ms):                          241253.91
Median TTFT (ms):                        221983.04
P99 TTFT (ms):                           495558.69
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          65.31
Median TPOT (ms):                        67.29
P99 TPOT (ms):                           67.82
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.33
Median ITL (ms):                         49.82
P99 ITL (ms):                            195.78
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 48.04s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [23:19<00:00,  2.80s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1399.64
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999812
Request throughput (req/s):              0.36
Input token throughput (tok/s):          714.47
Output token throughput (tok/s):         2143.40
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   892841.42
Median E2E Latency (ms):                 886623.87
---------------Time to First Token----------------
Mean TTFT (ms):                          353623.41
Median TTFT (ms):                        37579.07
P99 TTFT (ms):                           1023442.45
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          89.88
Median TPOT (ms):                        82.38
P99 TPOT (ms):                           141.69
---------------Inter-token Latency----------------
Mean ITL (ms):                           89.99
Median ITL (ms):                         62.26
P99 ITL (ms):                            294.10
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 16.79s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [14:05<00:00,  1.69s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  845.54
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999992
Request throughput (req/s):              0.59
Input token throughput (tok/s):          3548.02
Output token throughput (tok/s):         1182.67
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   549773.14
Median E2E Latency (ms):                 511145.17
---------------Time to First Token----------------
Mean TTFT (ms):                          345720.36
Median TTFT (ms):                        294033.84
P99 TTFT (ms):                           740690.53
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          102.08
Median TPOT (ms):                        107.43
P99 TPOT (ms):                           162.14
---------------Inter-token Latency----------------
Mean ITL (ms):                           102.16
Median ITL (ms):                         60.60
P99 ITL (ms):                            293.04
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## Qwen3-235B-A22B

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model Qwen/Qwen3-235B-A22B --tensor-parallel-size 8 \
    --host 0.0.0.0
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_serving.sh vllm
Using backend: vllm
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 3.64s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [13:47<00:00, 12.09it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  827.01
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991654
Request throughput (req/s):              12.09
Input token throughput (tok/s):          2731.02
Output token throughput (tok/s):         2408.89
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   409530.38
Median E2E Latency (ms):                 407529.96
---------------Time to First Token----------------
Mean TTFT (ms):                          389375.12
Median TTFT (ms):                        388252.76
P99 TTFT (ms):                           781008.84
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          104.95
Median TPOT (ms):                        101.30
P99 TPOT (ms):                           203.61
---------------Inter-token Latency----------------
Mean ITL (ms):                           101.84
Median ITL (ms):                         95.32
P99 ITL (ms):                            180.16
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 15.19s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [17:30<00:00,  1.90it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1050.01
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047747
Request throughput (req/s):              1.90
Input token throughput (tok/s):          1950.45
Output token throughput (tok/s):         1950.45
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   580254.56
Median E2E Latency (ms):                 568170.04
---------------Time to First Token----------------
Mean TTFT (ms):                          468514.66
Median TTFT (ms):                        446643.90
P99 TTFT (ms):                           976343.47
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          109.23
Median TPOT (ms):                        98.10
P99 TPOT (ms):                           172.22
---------------Inter-token Latency----------------
Mean ITL (ms):                           109.30
Median ITL (ms):                         73.89
P99 ITL (ms):                            235.84
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 91.78s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [48:58<00:00,  5.88s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2938.68
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999986
Request throughput (req/s):              0.17
Input token throughput (tok/s):          340.29
Output token throughput (tok/s):         1020.87
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1621670.22
Median E2E Latency (ms):                 1656150.32
---------------Time to First Token----------------
Mean TTFT (ms):                          1070268.29
Median TTFT (ms):                        1019769.66
P99 TTFT (ms):                           2400339.19
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          91.92
Median TPOT (ms):                        91.51
P99 TPOT (ms):                           161.47
---------------Inter-token Latency----------------
Mean ITL (ms):                           92.02
Median ITL (ms):                         54.71
P99 ITL (ms):                            232.96
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 31.83s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [24:05<00:00,  2.89s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1445.08
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999982
Request throughput (req/s):              0.35
Input token throughput (tok/s):          2076.01
Output token throughput (tok/s):         692.00
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   777371.92
Median E2E Latency (ms):                 766886.72
---------------Time to First Token----------------
Mean TTFT (ms):                          648933.69
Median TTFT (ms):                        646124.39
P99 TTFT (ms):                           1288353.20
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          64.25
Median TPOT (ms):                        60.90
P99 TPOT (ms):                           107.09
---------------Inter-token Latency----------------
Mean ITL (ms):                           64.31
Median ITL (ms):                         47.68
P99 ITL (ms):                            229.34
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## gpt-oss-20b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model openai/gpt-oss-20b --tensor-parallel-size 1 \
    --host 0.0.0.0
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_serving.sh vllm
Using backend: vllm
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2229766
#Output tokens: 1959361
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 4.43s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [06:19<00:00, 26.36it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  379.42
Total input tokens:                      2229766
Total generated tokens:                  1959361
Total generated tokens (retokenized):    1944977
Request throughput (req/s):              26.36
Input token throughput (tok/s):          5876.78
Output token throughput (tok/s):         5164.10
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   190588.37
Median E2E Latency (ms):                 189943.47
---------------Time to First Token----------------
Mean TTFT (ms):                          181443.13
Median TTFT (ms):                        181049.44
P99 TTFT (ms):                           359864.30
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          48.21
Median TPOT (ms):                        47.01
P99 TPOT (ms):                           90.54
---------------Inter-token Latency----------------
Mean ITL (ms):                           46.96
Median ITL (ms):                         37.35
P99 ITL (ms):                            110.20
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.72s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [07:21<00:00,  4.53it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  441.31
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1906243
Request throughput (req/s):              4.53
Input token throughput (tok/s):          4640.77
Output token throughput (tok/s):         4640.77
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   253112.49
Median E2E Latency (ms):                 239051.28
---------------Time to First Token----------------
Mean TTFT (ms):                          199083.58
Median TTFT (ms):                        183767.53
P99 TTFT (ms):                           404096.66
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          52.81
Median TPOT (ms):                        54.01
P99 TPOT (ms):                           55.22
---------------Inter-token Latency----------------
Mean ITL (ms):                           52.94
Median ITL (ms):                         41.23
P99 ITL (ms):                            136.31
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 39.28s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [14:51<00:00,  1.78s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  891.71
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2688225
Request throughput (req/s):              0.56
Input token throughput (tok/s):          1121.44
Output token throughput (tok/s):         3364.32
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   669859.72
Median E2E Latency (ms):                 485453.96
---------------Time to First Token----------------
Mean TTFT (ms):                          236225.47
Median TTFT (ms):                        37242.17
P99 TTFT (ms):                           486426.06
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          72.28
Median TPOT (ms):                        73.30
P99 TPOT (ms):                           74.72
---------------Inter-token Latency----------------
Mean ITL (ms):                           72.56
Median ITL (ms):                         69.85
P99 ITL (ms):                            178.58
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 13.64s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [10:04<00:00,  1.21s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  604.02
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    786126
Request throughput (req/s):              0.83
Input token throughput (tok/s):          4966.71
Output token throughput (tok/s):         1655.57
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   473105.44
Median E2E Latency (ms):                 454905.66
---------------Time to First Token----------------
Mean TTFT (ms):                          220394.29
Median TTFT (ms):                        162918.45
P99 TTFT (ms):                           459071.82
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          126.42
Median TPOT (ms):                        137.92
P99 TPOT (ms):                           146.07
---------------Inter-token Latency----------------
Mean ITL (ms):                           127.55
Median ITL (ms):                         86.33
P99 ITL (ms):                            279.93
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```

## gpt-oss-120b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "VLLM_LOGGING_LEVEL=ERROR" \
    -p 8000:8000 \
    --net host \
    --ipc=host --rm --name vllm \
    vllm/vllm-openai:v0.10.1 \
    --model openai/gpt-oss-120b --tensor-parallel-size 2 \
    --host 0.0.0.0
```

```{text}
(base) lvhan@HOST-10-140-24-141:~$ bash benchmark_serving.sh vllm
Using backend: vllm
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2229766
#Output tokens: 1959361
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 4.97s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:13<00:00, 20.26it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  493.49
Total input tokens:                      2229766
Total generated tokens:                  1959361
Total generated tokens (retokenized):    1917149
Request throughput (req/s):              20.26
Input token throughput (tok/s):          4518.39
Output token throughput (tok/s):         3970.44
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   245212.92
Median E2E Latency (ms):                 244736.09
---------------Time to First Token----------------
Mean TTFT (ms):                          233288.58
Median TTFT (ms):                        233224.01
P99 TTFT (ms):                           465438.45
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          62.35
Median TPOT (ms):                        61.38
P99 TPOT (ms):                           107.51
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.24
Median ITL (ms):                         50.35
P99 ITL (ms):                            131.37
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 9.08s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:04<00:00,  3.67it/s]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  544.87
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1984652
Request throughput (req/s):              3.67
Input token throughput (tok/s):          3758.69
Output token throughput (tok/s):         3758.69
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   308765.67
Median E2E Latency (ms):                 290523.22
---------------Time to First Token----------------
Mean TTFT (ms):                          242108.89
Median TTFT (ms):                        223256.94
P99 TTFT (ms):                           490674.40
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          65.16
Median TPOT (ms):                        65.98
P99 TPOT (ms):                           66.52
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.29
Median ITL (ms):                         52.63
P99 ITL (ms):                            153.59
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 53.40s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [15:55<00:00,  1.91s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  955.25
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2865560
Request throughput (req/s):              0.52
Input token throughput (tok/s):          1046.85
Output token throughput (tok/s):         3140.55
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   716530.67
Median E2E Latency (ms):                 518947.70
---------------Time to First Token----------------
Mean TTFT (ms):                          252831.64
Median TTFT (ms):                        40690.75
P99 TTFT (ms):                           520688.03
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          77.30
Median TPOT (ms):                        78.33
P99 TPOT (ms):                           79.72
---------------Inter-token Latency----------------
Mean ITL (ms):                           77.91
Median ITL (ms):                         74.09
P99 ITL (ms):                            191.69
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: vllm
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='vllm', base_url=None, host='0.0.0.0', port=8000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 18.13s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [10:12<00:00,  1.23s/it]

============ Serving Benchmark Result ============
Backend:                                 vllm
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  612.86
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    978464
Request throughput (req/s):              0.82
Input token throughput (tok/s):          4895.06
Output token throughput (tok/s):         1631.69
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   476317.06
Median E2E Latency (ms):                 453026.26
---------------Time to First Token----------------
Mean TTFT (ms):                          220096.08
Median TTFT (ms):                        161133.91
P99 TTFT (ms):                           457142.02
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          128.17
Median TPOT (ms):                        138.67
P99 TPOT (ms):                           146.02
---------------Inter-token Latency----------------
Mean ITL (ms):                           128.78
Median ITL (ms):                         89.86
P99 ITL (ms):                            264.07
==================================================
----------------------------------------
All benchmarks completed for backend: vllm
```
