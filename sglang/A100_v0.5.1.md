# SGLang v0.5.1

Prepare the docker image:

```shell
docker pull lmsysorg/sglang:v0.5.1-cu126
```

Serve a model via docker:

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc=host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path ${MODEL_REPO_ID} --tp ${TP} --host 0.0.0.0
```

## Table of Contents

- [SGLang v0.10.1](#sglang-v0101)
  - [Table of Contents](#table-of-contents)
  - [Qwen3-8B](#qwen3-8b)
  - [Qwen3-32B](#qwen3-32b)
  - [Qwen3-30B-A3B](#qwen3-30b-a3b)
  - [Qwen3-235B-A22B](#qwen3-235b-a22b)
  - [gpt-oss-20b](#gpt-oss-20b)
  - [gpt-oss-120b](#gpt-oss-120b)

<!-- tocstop -->

## Qwen3-8B

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc=host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B --tp 1 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 3.05s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [07:03<00:00, 23.62it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  423.29
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1992131
Request throughput (req/s):              23.62
Input token throughput (tok/s):          5335.81
Output token throughput (tok/s):         4706.44
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   225447.14
Median E2E Latency (ms):                 225201.90
---------------Time to First Token----------------
Mean TTFT (ms):                          188520.68
Median TTFT (ms):                        187153.72
P99 TTFT (ms):                           378786.34
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          230.14
Median TPOT (ms):                        193.10
P99 TPOT (ms):                           1221.51
---------------Inter-token Latency----------------
Mean ITL (ms):                           186.29
Median ITL (ms):                         168.38
P99 ITL (ms):                            754.93
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 11.84s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [10:40<00:00,  3.12it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  640.59
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2046478
Request throughput (req/s):              3.12
Input token throughput (tok/s):          3197.08
Output token throughput (tok/s):         3197.08
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   360656.30
Median E2E Latency (ms):                 388845.90
---------------Time to First Token----------------
Mean TTFT (ms):                          274601.85
Median TTFT (ms):                        307467.30
P99 TTFT (ms):                           573614.19
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          84.12
Median TPOT (ms):                        55.24
P99 TPOT (ms):                           566.79
---------------Inter-token Latency----------------
Mean ITL (ms):                           84.11
Median ITL (ms):                         48.53
P99 ITL (ms):                            65.62
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 71.46s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [34:13<00:00,  4.11s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2053.66
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2998038
Request throughput (req/s):              0.24
Input token throughput (tok/s):          486.93
Output token throughput (tok/s):         1460.80
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1300044.37
Median E2E Latency (ms):                 1486592.09
---------------Time to First Token----------------
Mean TTFT (ms):                          776800.75
Median TTFT (ms):                        748940.44
P99 TTFT (ms):                           1741822.33
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          87.22
Median TPOT (ms):                        42.13
P99 TPOT (ms):                           317.46
---------------Inter-token Latency----------------
Mean ITL (ms):                           87.22
Median ITL (ms):                         42.65
P99 ITL (ms):                            48.01
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 24.71s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [17:47<00:00,  2.13s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1067.12
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    998991
Request throughput (req/s):              0.47
Input token throughput (tok/s):          2811.29
Output token throughput (tok/s):         937.10
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   598707.91
Median E2E Latency (ms):                 642098.43
---------------Time to First Token----------------
Mean TTFT (ms):                          475466.68
Median TTFT (ms):                        440623.00
P99 TTFT (ms):                           966007.95
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          61.65
Median TPOT (ms):                        48.41
P99 TPOT (ms):                           425.41
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.65
Median ITL (ms):                         42.13
P99 ITL (ms):                            47.48
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## Qwen3-32B

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc=host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-32B --tp 2 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.18s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [14:21<00:00, 11.60it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  861.96
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991625
Request throughput (req/s):              11.60
Input token throughput (tok/s):          2620.29
Output token throughput (tok/s):         2311.22
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   450979.60
Median E2E Latency (ms):                 451179.14
---------------Time to First Token----------------
Mean TTFT (ms):                          390704.23
Median TTFT (ms):                        389434.10
P99 TTFT (ms):                           792968.66
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          358.68
Median TPOT (ms):                        313.06
P99 TPOT (ms):                           1749.64
---------------Inter-token Latency----------------
Mean ITL (ms):                           304.09
Median ITL (ms):                         284.78
P99 ITL (ms):                            896.08
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 24.80s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [19:57<00:00,  1.67it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1197.22
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047832
Request throughput (req/s):              1.67
Input token throughput (tok/s):          1710.64
Output token throughput (tok/s):         1710.64
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   672692.87
Median E2E Latency (ms):                 655121.36
---------------Time to First Token----------------
Mean TTFT (ms):                          475860.79
Median TTFT (ms):                        487408.17
P99 TTFT (ms):                           955903.69
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          192.41
Median TPOT (ms):                        80.50
P99 TPOT (ms):                           952.76
---------------Inter-token Latency----------------
Mean ITL (ms):                           192.37
Median ITL (ms):                         64.96
P99 ITL (ms):                            202.45
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 147.07s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [52:10<00:00,  6.26s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3130.34
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2996413
Request throughput (req/s):              0.16
Input token throughput (tok/s):          319.45
Output token throughput (tok/s):         958.36
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1942195.89
Median E2E Latency (ms):                 2029852.34
---------------Time to First Token----------------
Mean TTFT (ms):                          1210838.25
Median TTFT (ms):                        1165656.93
P99 TTFT (ms):                           2595915.54
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          121.91
Median TPOT (ms):                        47.15
P99 TPOT (ms):                           461.50
---------------Inter-token Latency----------------
Mean ITL (ms):                           121.91
Median ITL (ms):                         47.67
P99 ITL (ms):                            54.07
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 50.51s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [28:48<00:00,  3.46s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1728.19
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999642
Request throughput (req/s):              0.29
Input token throughput (tok/s):          1735.92
Output token throughput (tok/s):         578.64
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   944260.78
Median E2E Latency (ms):                 908187.88
---------------Time to First Token----------------
Mean TTFT (ms):                          794650.07
Median TTFT (ms):                        799083.63
P99 TTFT (ms):                           1598967.05
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          74.84
Median TPOT (ms):                        56.99
P99 TPOT (ms):                           597.27
---------------Inter-token Latency----------------
Mean ITL (ms):                           74.84
Median ITL (ms):                         46.88
P99 ITL (ms):                            49.73
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## Qwen3-30B-A3B

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc=host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --tp 2 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.57s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:32<00:00, 36.65it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  272.87
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1992111
Request throughput (req/s):              36.65
Input token throughput (tok/s):          8277.13
Output token throughput (tok/s):         7300.82
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   147350.72
Median E2E Latency (ms):                 149092.79
---------------Time to First Token----------------
Mean TTFT (ms):                          102892.04
Median TTFT (ms):                        105495.32
P99 TTFT (ms):                           207085.10
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          344.57
Median TPOT (ms):                        232.48
P99 TPOT (ms):                           2259.66
---------------Inter-token Latency----------------
Mean ITL (ms):                           224.31
Median ITL (ms):                         117.28
P99 ITL (ms):                            1023.90
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.31s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:44<00:00,  4.95it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  404.39
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047911
Request throughput (req/s):              4.95
Input token throughput (tok/s):          5064.45
Output token throughput (tok/s):         5064.45
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   244319.11
Median E2E Latency (ms):                 253274.41
---------------Time to First Token----------------
Mean TTFT (ms):                          147870.33
Median TTFT (ms):                        174944.55
P99 TTFT (ms):                           271110.60
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          94.28
Median TPOT (ms):                        80.56
P99 TPOT (ms):                           348.97
---------------Inter-token Latency----------------
Mean ITL (ms):                           94.26
Median ITL (ms):                         72.54
P99 ITL (ms):                            296.29
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 37.02s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [19:41<00:00,  2.36s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1181.98
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999970
Request throughput (req/s):              0.42
Input token throughput (tok/s):          846.04
Output token throughput (tok/s):         2538.12
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   814418.33
Median E2E Latency (ms):                 948541.16
---------------Time to First Token----------------
Mean TTFT (ms):                          371270.65
Median TTFT (ms):                        369735.86
P99 TTFT (ms):                           950601.61
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          73.87
Median TPOT (ms):                        48.81
P99 TPOT (ms):                           183.12
---------------Inter-token Latency----------------
Mean ITL (ms):                           73.87
Median ITL (ms):                         49.61
P99 ITL (ms):                            81.03
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 12.73s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:36<00:00,  1.03s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  516.33
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999977
Request throughput (req/s):              0.97
Input token throughput (tok/s):          5810.26
Output token throughput (tok/s):         1936.75
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   311198.64
Median E2E Latency (ms):                 326115.11
---------------Time to First Token----------------
Mean TTFT (ms):                          208413.56
Median TTFT (ms):                        227128.32
P99 TTFT (ms):                           451019.78
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          51.42
Median TPOT (ms):                        49.17
P99 TPOT (ms):                           221.16
---------------Inter-token Latency----------------
Mean ITL (ms):                           51.42
Median ITL (ms):                         43.29
P99 ITL (ms):                            50.96
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## Qwen3-235B-A22B

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc=host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B --tp 8 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 3.44s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [13:23<00:00, 12.45it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  803.52
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991671
Request throughput (req/s):              12.45
Input token throughput (tok/s):          2810.86
Output token throughput (tok/s):         2479.31
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   405753.93
Median E2E Latency (ms):                 406075.35
---------------Time to First Token----------------
Mean TTFT (ms):                          368689.45
Median TTFT (ms):                        367578.75
P99 TTFT (ms):                           741678.04
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          199.70
Median TPOT (ms):                        190.49
P99 TPOT (ms):                           558.25
---------------Inter-token Latency----------------
Mean ITL (ms):                           186.99
Median ITL (ms):                         209.62
P99 ITL (ms):                            671.75
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 13.67s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [21:11<00:00,  1.57it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  1272.00
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2045813
Request throughput (req/s):              1.57
Input token throughput (tok/s):          1610.06
Output token throughput (tok/s):         1610.06
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   719849.40
Median E2E Latency (ms):                 749841.56
---------------Time to First Token----------------
Mean TTFT (ms):                          521946.25
Median TTFT (ms):                        478918.06
P99 TTFT (ms):                           1037071.41
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          193.45
Median TPOT (ms):                        69.56
P99 TPOT (ms):                           1011.00
---------------Inter-token Latency----------------
Mean ITL (ms):                           193.41
Median ITL (ms):                         54.59
P99 ITL (ms):                            177.10
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 80.01s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:00:54<00:00,  7.31s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3654.09
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999319
Request throughput (req/s):              0.14
Input token throughput (tok/s):          273.67
Output token throughput (tok/s):         821.00
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2287701.18
Median E2E Latency (ms):                 2507559.48
---------------Time to First Token----------------
Mean TTFT (ms):                          1532860.31
Median TTFT (ms):                        1603219.96
P99 TTFT (ms):                           3178510.70
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          125.83
Median TPOT (ms):                        37.95
P99 TPOT (ms):                           545.14
---------------Inter-token Latency----------------
Mean ITL (ms):                           125.82
Median ITL (ms):                         36.95
P99 ITL (ms):                            40.97
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 27.35s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [27:55<00:00,  3.35s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1675.15
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999962
Request throughput (req/s):              0.30
Input token throughput (tok/s):          1790.88
Output token throughput (tok/s):         596.96
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   906039.57
Median E2E Latency (ms):                 926415.72
---------------Time to First Token----------------
Mean TTFT (ms):                          788625.86
Median TTFT (ms):                        764008.00
P99 TTFT (ms):                           1590737.35
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          58.74
Median TPOT (ms):                        38.82
P99 TPOT (ms):                           626.86
---------------Inter-token Latency----------------
Mean ITL (ms):                           58.73
Median ITL (ms):                         36.07
P99 ITL (ms):                            38.94
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## gpt-oss-20b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc=host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path openai/gpt-oss-20b --tp 1 --host 0.0.0.0
```

Note that in [benchmark_serving.sh](../benchmark_serving.sh), the dataset settings have been changed to `--dataset-name sharegpt --num-prompts 10000` to prevent crashes.

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 1000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=1000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 221351
#Output tokens: 191137
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 2.72s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [11:59<00:00,  1.39it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     1000
Benchmark duration (s):                  719.19
Total input tokens:                      221351
Total generated tokens:                  191137
Total generated tokens (retokenized):    189600
Request throughput (req/s):              1.39
Input token throughput (tok/s):          307.78
Output token throughput (tok/s):         265.77
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   505409.55
Median E2E Latency (ms):                 499764.77
---------------Time to First Token----------------
Mean TTFT (ms):                          153727.14
Median TTFT (ms):                        153625.07
P99 TTFT (ms):                           290360.86
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          6661.16
Median TPOT (ms):                        2645.47
P99 TPOT (ms):                           54241.65
---------------Inter-token Latency----------------
Mean ITL (ms):                           1849.63
Median ITL (ms):                         1142.78
P99 ITL (ms):                            5173.86
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 9.08s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [2:03:43<00:00,  3.71s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  7423.11
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1909081
Request throughput (req/s):              0.27
Input token throughput (tok/s):          275.90
Output token throughput (tok/s):         275.90
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   5010590.31
Median E2E Latency (ms):                 4719353.73
---------------Time to First Token----------------
Mean TTFT (ms):                          2884124.67
Median TTFT (ms):                        2846742.63
P99 TTFT (ms):                           7076226.03
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          2078.66
Median TPOT (ms):                        1894.18
P99 TPOT (ms):                           6594.36
---------------Inter-token Latency----------------
Mean ITL (ms):                           2078.59
Median ITL (ms):                         1489.20
P99 ITL (ms):                            1842.40
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 43.38s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:00:44<00:00,  7.29s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3644.76
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2708862
Request throughput (req/s):              0.14
Input token throughput (tok/s):          274.37
Output token throughput (tok/s):         823.10
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2352603.48
Median E2E Latency (ms):                 2079894.39
---------------Time to First Token----------------
Mean TTFT (ms):                          937676.75
Median TTFT (ms):                        857064.43
P99 TTFT (ms):                           2203043.34
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          235.86
Median TPOT (ms):                        159.73
P99 TPOT (ms):                           552.22
---------------Inter-token Latency----------------
Mean ITL (ms):                           235.85
Median ITL (ms):                         76.43
P99 ITL (ms):                            111.24
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-20b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 22.14s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:11:16<00:00,  8.55s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  4276.49
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    812961
Request throughput (req/s):              0.12
Input token throughput (tok/s):          701.51
Output token throughput (tok/s):         233.84
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2743116.31
Median E2E Latency (ms):                 2613591.88
---------------Time to First Token----------------
Mean TTFT (ms):                          2068647.23
Median TTFT (ms):                        2042548.85
P99 TTFT (ms):                           4226970.73
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          337.40
Median TPOT (ms):                        303.82
P99 TPOT (ms):                           1540.47
---------------Inter-token Latency----------------
Mean ITL (ms):                           337.42
Median ITL (ms):                         51.10
P99 ITL (ms):                            68.68
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## gpt-oss-120b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc=host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --tp 2 --host 0.0.0.0
```

Note that in [benchmark_serving.sh](../benchmark_serving.sh), the dataset settings have been changed to `--dataset-name sharegpt --num-prompts 10000` to prevent crashes.

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.55.4)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)
Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)
Running benchmark --dataset-name sharegpt --num-prompts 1000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=1000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 221351
#Output tokens: 191137
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 4.50s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:42<00:00,  2.48it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     1000
Benchmark duration (s):                  402.66
Total input tokens:                      221351
Total generated tokens:                  191137
Total generated tokens (retokenized):    185832
Request throughput (req/s):              2.48
Input token throughput (tok/s):          549.73
Output token throughput (tok/s):         474.69
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   335887.09
Median E2E Latency (ms):                 333418.31
---------------Time to First Token----------------
Mean TTFT (ms):                          151714.06
Median TTFT (ms):                        155694.53
P99 TTFT (ms):                           276903.54
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          5420.30
Median TPOT (ms):                        1426.17
P99 TPOT (ms):                           50283.12
---------------Inter-token Latency----------------
Mean ITL (ms):                           968.63
Median ITL (ms):                         89.54
P99 ITL (ms):                            2676.78
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 8.46s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [50:36<00:00,  1.52s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  3036.03
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    1982777
Request throughput (req/s):              0.66
Input token throughput (tok/s):          674.57
Output token throughput (tok/s):         674.57
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1902244.51
Median E2E Latency (ms):                 1590476.46
---------------Time to First Token----------------
Mean TTFT (ms):                          1346190.55
Median TTFT (ms):                        1309843.22
P99 TTFT (ms):                           2711623.86
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          543.55
Median TPOT (ms):                        470.62
P99 TPOT (ms):                           2298.91
---------------Inter-token Latency----------------
Mean ITL (ms):                           543.54
Median ITL (ms):                         105.35
P99 ITL (ms):                            367.63
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 51.84s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:01:08<00:00,  7.34s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3668.75
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2886842
Request throughput (req/s):              0.14
Input token throughput (tok/s):          272.57
Output token throughput (tok/s):         817.72
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2434872.94
Median E2E Latency (ms):                 1953784.64
---------------Time to First Token----------------
Mean TTFT (ms):                          1008063.05
Median TTFT (ms):                        1118814.42
P99 TTFT (ms):                           2583841.15
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          237.84
Median TPOT (ms):                        164.88
P99 TPOT (ms):                           544.02
---------------Inter-token Latency----------------
Mean ITL (ms):                           237.83
Median ITL (ms):                         79.35
P99 ITL (ms):                            120.76
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme/qa_test_models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json', model='openai/gpt-oss-120b', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 24.05s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:09:17<00:00,  8.32s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  4157.83
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    972391
Request throughput (req/s):              0.12
Input token throughput (tok/s):          721.53
Output token throughput (tok/s):         240.51
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2608408.06
Median E2E Latency (ms):                 2236832.75
---------------Time to First Token----------------
Mean TTFT (ms):                          1986409.96
Median TTFT (ms):                        1974588.95
P99 TTFT (ms):                           4046582.02
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          311.15
Median TPOT (ms):                        280.98
P99 TPOT (ms):                           1673.89
---------------Inter-token Latency----------------
Mean ITL (ms):                           311.17
Median ITL (ms):                         68.45
P99 ITL (ms):                            72.43
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```
