# SGLang v0.5.1

Prepare the docker image:

```shell
docker pull lmsysorg/sglang:v0.5.1-cu126
```

Serve a model via docker:

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    -p 30000:30000 \
    --net host \
    --ipc host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path ${MODEL_REPO_ID} --tp ${TP} --host 0.0.0.0
```

## Table of Contents

- [SGLang v0.10.1](#sglang-v0101)
  - [Table of Contents](#table-of-contents)
  - [Qwen3-8B-FP8](#qwen3-8b-fp8)
  - [Qwen3-32B-FP8](#qwen3-32b-fp8)
  - [Qwen3-30B-A3B-FP8](#qwen3-30b-a3b-fp8)
  - [Qwen3-235B-A22B-FP8](#qwen3-235b-a22b-fp8)
  - [gpt-oss-20b](#gpt-oss-20b)
  - [gpt-oss-120b](#gpt-oss-120b)

<!-- tocstop -->

## Qwen3-8B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B-FP8 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.54s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [03:09<00:00, 52.77it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  189.51
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1992131
Request throughput (req/s):              52.77
Input token throughput (tok/s):          11917.70
Output token throughput (tok/s):         10511.98
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   91077.79
Median E2E Latency (ms):                 91145.18
---------------Time to First Token----------------
Mean TTFT (ms):                          72965.18
Median TTFT (ms):                        69314.32
P99 TTFT (ms):                           158452.39
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          92.58
Median TPOT (ms):                        97.26
P99 TPOT (ms):                           142.88
---------------Inter-token Latency----------------
Mean ITL (ms):                           91.38
Median ITL (ms):                         63.01
P99 ITL (ms):                            658.65
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 6.37s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [07:17<00:00,  4.57it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  437.21
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047975
Request throughput (req/s):              4.57
Input token throughput (tok/s):          4684.23
Output token throughput (tok/s):         4684.23
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   262118.32
Median E2E Latency (ms):                 262922.07
---------------Time to First Token----------------
Mean TTFT (ms):                          169821.32
Median TTFT (ms):                        160394.09
P99 TTFT (ms):                           364685.30
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          90.22
Median TPOT (ms):                        48.60
P99 TPOT (ms):                           360.01
---------------Inter-token Latency----------------
Mean ITL (ms):                           90.20
Median ITL (ms):                         45.82
P99 ITL (ms):                            56.34
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 38.24s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [17:27<00:00,  2.10s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1047.94
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2997124
Request throughput (req/s):              0.48
Input token throughput (tok/s):          954.25
Output token throughput (tok/s):         2862.76
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   675533.98
Median E2E Latency (ms):                 719799.11
---------------Time to First Token----------------
Mean TTFT (ms):                          387935.53
Median TTFT (ms):                        436349.93
P99 TTFT (ms):                           862471.52
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.94
Median TPOT (ms):                        23.64
P99 TPOT (ms):                           156.96
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.94
Median ITL (ms):                         25.43
P99 ITL (ms):                            29.81
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-8B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 13.11s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:37<00:00,  1.03s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  517.25
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    998848
Request throughput (req/s):              0.97
Input token throughput (tok/s):          5799.89
Output token throughput (tok/s):         1933.30
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   291495.85
Median E2E Latency (ms):                 286022.16
---------------Time to First Token----------------
Mean TTFT (ms):                          230650.62
Median TTFT (ms):                        232208.78
P99 TTFT (ms):                           463283.16
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          30.44
Median TPOT (ms):                        26.92
P99 TPOT (ms):                           179.26
---------------Inter-token Latency----------------
Mean ITL (ms):                           30.44
Median ITL (ms):                         25.24
P99 ITL (ms):                            28.09
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## Qwen3-32B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-32B-FP8 --tp 1 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 4.28s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [11:05<00:00, 15.03it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  665.36
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1989602
Request throughput (req/s):              15.03
Input token throughput (tok/s):          3394.53
Output token throughput (tok/s):         2994.14
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   329119.67
Median E2E Latency (ms):                 328372.24
---------------Time to First Token----------------
Mean TTFT (ms):                          305296.55
Median TTFT (ms):                        300712.30
P99 TTFT (ms):                           627631.77
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          118.80
Median TPOT (ms):                        122.67
P99 TPOT (ms):                           151.98
---------------Inter-token Latency----------------
Mean ITL (ms):                           120.19
Median ITL (ms):                         148.43
P99 ITL (ms):                            438.05
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 17.92s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [15:59<00:00,  2.08it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  959.43
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047724
Request throughput (req/s):              2.08
Input token throughput (tok/s):          2134.60
Output token throughput (tok/s):         2134.60
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   534939.49
Median E2E Latency (ms):                 541076.58
---------------Time to First Token----------------
Mean TTFT (ms):                          381792.66
Median TTFT (ms):                        388731.21
P99 TTFT (ms):                           778218.56
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          149.70
Median TPOT (ms):                        37.36
P99 TPOT (ms):                           718.52
---------------Inter-token Latency----------------
Mean ITL (ms):                           149.67
Median ITL (ms):                         32.17
P99 ITL (ms):                            112.60
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 106.84s
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [1:02:03<00:00,  7.45s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  3723.47
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2997678
Request throughput (req/s):              0.13
Input token throughput (tok/s):          268.57
Output token throughput (tok/s):         805.70
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2290478.79
Median E2E Latency (ms):                 2411373.99
---------------Time to First Token----------------
Mean TTFT (ms):                          1544916.08
Median TTFT (ms):                        1551341.36
P99 TTFT (ms):                           3103294.59
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          124.28
Median TPOT (ms):                        28.60
P99 TPOT (ms):                           557.16
---------------Inter-token Latency----------------
Mean ITL (ms):                           124.27
Median ITL (ms):                         28.25
P99 ITL (ms):                            34.19
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-32B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 36.50s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [32:52<00:00,  3.94s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1972.20
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999348
Request throughput (req/s):              0.25
Input token throughput (tok/s):          1521.15
Output token throughput (tok/s):         507.05
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1049186.08
Median E2E Latency (ms):                 1027882.41
---------------Time to First Token----------------
Mean TTFT (ms):                          926633.01
Median TTFT (ms):                        954435.39
P99 TTFT (ms):                           1843511.16
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          61.31
Median TPOT (ms):                        34.33
P99 TPOT (ms):                           775.93
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.31
Median ITL (ms):                         32.19
P99 ITL (ms):                            34.38
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## Qwen3-30B-A3B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-FP8 --tp 1 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 1.40s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:50<00:00, 34.47it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  290.12
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991978
Request throughput (req/s):              34.47
Input token throughput (tok/s):          7785.10
Output token throughput (tok/s):         6866.83
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   136741.75
Median E2E Latency (ms):                 135782.61
---------------Time to First Token----------------
Mean TTFT (ms):                          108949.52
Median TTFT (ms):                        105517.68
P99 TTFT (ms):                           243225.75
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          138.12
Median TPOT (ms):                        150.98
P99 TPOT (ms):                           193.97
---------------Inter-token Latency----------------
Mean ITL (ms):                           140.21
Median ITL (ms):                         152.68
P99 ITL (ms):                            713.48
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 5.62s
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [11:06<00:00,  3.00it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  666.73
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047914
Request throughput (req/s):              3.00
Input token throughput (tok/s):          3071.73
Output token throughput (tok/s):         3071.73
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   399318.02
Median E2E Latency (ms):                 396108.42
---------------Time to First Token----------------
Mean TTFT (ms):                          256982.54
Median TTFT (ms):                        240829.79
P99 TTFT (ms):                           551938.30
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          139.14
Median TPOT (ms):                        75.10
P99 TPOT (ms):                           548.25
---------------Inter-token Latency----------------
Mean ITL (ms):                           139.11
Median ITL (ms):                         72.38
P99 ITL (ms):                            89.52
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 33.69s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [18:48<00:00,  2.26s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1128.08
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999725
Request throughput (req/s):              0.44
Input token throughput (tok/s):          886.46
Output token throughput (tok/s):         2659.39
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   724746.76
Median E2E Latency (ms):                 767965.66
---------------Time to First Token----------------
Mean TTFT (ms):                          423539.92
Median TTFT (ms):                        461855.36
P99 TTFT (ms):                           921610.32
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          50.21
Median TPOT (ms):                        25.62
P99 TPOT (ms):                           166.81
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.21
Median ITL (ms):                         25.53
P99 ITL (ms):                            30.05
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 11.53s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:23<00:00,  1.01s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  503.83
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999976
Request throughput (req/s):              0.99
Input token throughput (tok/s):          5954.36
Output token throughput (tok/s):         1984.79
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   287088.39
Median E2E Latency (ms):                 283076.00
---------------Time to First Token----------------
Mean TTFT (ms):                          222325.35
Median TTFT (ms):                        224506.73
P99 TTFT (ms):                           449675.25
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          32.40
Median TPOT (ms):                        25.81
P99 TPOT (ms):                           196.48
---------------Inter-token Latency----------------
Mean ITL (ms):                           32.40
Median ITL (ms):                         24.48
P99 ITL (ms):                            29.76
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## Qwen3-235B-A22B-FP8

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --tp 4 --host 0.0.0.0
```

```{text}
$ bash benchmark_serving.sh sglang
Using backend: sglang
Looking in indexes: https://mirrors.aliyun.com/pypi/simple/
Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.13/site-packages (3.12.15)
Requirement already satisfied: numpy in ./miniconda3/lib/python3.13/site-packages (2.3.2)
Requirement already satisfied: transformers in ./miniconda3/lib/python3.13/site-packages (4.56.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (6.6.4)
Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/lib/python3.13/site-packages (from aiohttp) (1.20.1)
Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)
Requirement already satisfied: filelock in ./miniconda3/lib/python3.13/site-packages (from transformers) (3.19.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)
Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (24.2)
Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.13/site-packages (from transformers) (2025.8.29)
Requirement already satisfied: requests in ./miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.13/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)
Running benchmark --dataset-name sharegpt --num-prompts 10000 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='sharegpt', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=10000, sharegpt_output_len=None, random_input_len=None, random_output_len=None, random_range_ratio=0.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2258583
#Output tokens: 1992177
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 3.29s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [15:21<00:00, 10.85it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     10000
Benchmark duration (s):                  921.79
Total input tokens:                      2258583
Total generated tokens:                  1992177
Total generated tokens (retokenized):    1991699
Request throughput (req/s):              10.85
Input token throughput (tok/s):          2450.21
Output token throughput (tok/s):         2161.20
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   454852.41
Median E2E Latency (ms):                 452647.92
---------------Time to First Token----------------
Mean TTFT (ms):                          402121.82
Median TTFT (ms):                        400156.90
P99 TTFT (ms):                           849684.28
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          264.70
Median TPOT (ms):                        273.76
P99 TPOT (ms):                           319.84
---------------Inter-token Latency----------------
Mean ITL (ms):                           266.02
Median ITL (ms):                         304.97
P99 ITL (ms):                            748.77
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 2000 --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=2000, sharegpt_output_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 2048000
#Output tokens: 2048000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 12.88s
 47%|█████████████████████████████████████████████████████████▋                                                                 | 938/2000 [08:22<10:19,  1.71it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [14:28<00:00,  2.30it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     2000
Benchmark duration (s):                  868.91
Total input tokens:                      2048000
Total generated tokens:                  2048000
Total generated tokens (retokenized):    2047831
Request throughput (req/s):              2.30
Input token throughput (tok/s):          2356.97
Output token throughput (tok/s):         2356.97
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   486471.06
Median E2E Latency (ms):                 502079.43
---------------Time to First Token----------------
Mean TTFT (ms):                          367183.69
Median TTFT (ms):                        361856.03
P99 TTFT (ms):                           774112.03
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          116.61
Median TPOT (ms):                        39.74
P99 TPOT (ms):                           717.66
---------------Inter-token Latency----------------
Mean ITL (ms):                           116.59
Median ITL (ms):                         36.58
P99 ITL (ms):                            152.44
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 2000 --random-output-len 6000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=2000, random_output_len=6000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 1000000
#Output tokens: 3000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 75.90s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [37:11<00:00,  4.46s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  2231.91
Total input tokens:                      1000000
Total generated tokens:                  3000000
Total generated tokens (retokenized):    2999460
Request throughput (req/s):              0.22
Input token throughput (tok/s):          448.05
Output token throughput (tok/s):         1344.14
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1404065.15
Median E2E Latency (ms):                 1538642.20
---------------Time to First Token----------------
Mean TTFT (ms):                          907475.27
Median TTFT (ms):                        858352.93
P99 TTFT (ms):                           1885355.93
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          82.78
Median TPOT (ms):                        29.14
P99 TPOT (ms):                           327.52
---------------Inter-token Latency----------------
Mean ITL (ms):                           82.77
Median ITL (ms):                         27.89
P99 ITL (ms):                            31.32
==================================================
----------------------------------------
Running benchmark --dataset-name random --num-prompts 500 --random-input-len 6000 --random-output-len 2000 --random-range-ratio 1.0 with backend: sglang
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='random', dataset_path='/nvme1/shared/ShareGPT_V3_unfiltered_cleaned_split.json', model='Qwen/Qwen3-235B-A22B-FP8', tokenizer=None, num_prompts=500, sharegpt_output_len=None, random_input_len=6000, random_output_len=2000, random_range_ratio=1.0, request_rate=inf, seed=1, multi=False, request_rate_range='2,34,2', output_file=None, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, extra_request_body=None, disable_warmup=None)

#Input tokens: 3000000
#Output tokens: 1000000
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
warmup time: 25.89s
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [16:58<00:00,  2.04s/it]

============ Serving Benchmark Result ============
Backend:                                 sglang
Traffic request rate:                    inf
Successful requests:                     500
Benchmark duration (s):                  1018.70
Total input tokens:                      3000000
Total generated tokens:                  1000000
Total generated tokens (retokenized):    999952
Request throughput (req/s):              0.49
Input token throughput (tok/s):          2944.92
Output token throughput (tok/s):         981.64
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   553515.88
Median E2E Latency (ms):                 559344.37
---------------Time to First Token----------------
Mean TTFT (ms):                          453153.33
Median TTFT (ms):                        437454.63
P99 TTFT (ms):                           941086.58
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          50.21
Median TPOT (ms):                        29.02
P99 TPOT (ms):                           415.86
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.20
Median ITL (ms):                         26.30
P99 ITL (ms):                            29.90
==================================================
----------------------------------------
All benchmarks completed for backend: sglang
```

## gpt-oss-20b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path openai/gpt-oss-20b --host 0.0.0.0
```

```{text}
```

## gpt-oss-120b

```shell
docker run --runtime nvidia --gpus all \
    -v $HF_HOME:/root/.cache/huggingface_hub \
    --env "HF_HOME=/root/.cache/huggingface_hub" \
    --env "TRANSFORMERS_OFFLINE=1" \
    -p 30000:30000 \
    --net host \
    --ipc host --rm --name sglang \
    lmsysorg/sglang:v0.5.1-cu126 \
    python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --host 0.0.0.0
```

```{text}
```
